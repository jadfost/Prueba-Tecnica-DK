{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ae9f80-6593-4251-a371-d358c173fe09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importé las bibliotecas necesarias\n",
    "import boto3  # Para interactuar con AWS (Amazon Web Services)\n",
    "import pandas as pd  # Para el análisis de datos\n",
    "from io import StringIO  # Para leer y escribir cadenas como archivos\n",
    "\n",
    "# Creé una instancia del cliente de S3\n",
    "s3_client = boto3.client('s3')  # Inicialicé el cliente de S3 para interactuar con el servicio de almacenamiento de Amazon S3\n",
    "\n",
    "# Descargué el archivo CSV desde S3\n",
    "csv_file = s3_client.get_object(Bucket='modelaciondk', Key='tickitdb/merge_data.csv')['Body'].read().decode('utf-8')\n",
    "# Utilicé el cliente de S3 para obtener el objeto (archivo) desde el bucket 'modelaciondk' y la clave 'tickitdb/merge_data.csv'\n",
    "# Leí el contenido del archivo, lo decodifiqué del formato binario y lo almacené en la variable csv_file\n",
    "\n",
    "# Convertí el CSV a DataFrame con nombres de columnas asignados\n",
    "column_names = ['User', 'LastName', 'Email', 'EventName', 'EventLocation', 'EventDate', 'Quantity', 'TotalSold']\n",
    "df = pd.read_csv(StringIO(csv_file), header=None, names=column_names)\n",
    "# Utilicé pandas para leer el contenido del archivo CSV almacenado en csv_file\n",
    "# Asigné nombres a las columnas del DataFrame según la lista column_names\n",
    "\n",
    "# Aseguré de que la columna de fecha fuera del tipo datetime\n",
    "df['EventDate'] = pd.to_datetime(df['EventDate'])\n",
    "# Convertí la columna 'EventDate' del DataFrame al tipo de dato datetime, facilitando el manejo de fechas en análisis posteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "006f7c91-8aea-47cb-90ea-8e00d5e2980d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.34.19)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.19 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3) (1.34.19)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.19->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.19->boto3) (1.26.18)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.19->boto3) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38df09e6-32c3-4aca-982f-d8fbc0f7f5c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "WARNING:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.0.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: sales-forecast-training-job-1705802507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-21 02:01:48 Starting - Starting the training job...\n",
      "2024-01-21 02:02:13 Starting - Preparing the instances for training.........\n",
      "2024-01-21 02:03:37 Downloading - Downloading input data...\n",
      "2024-01-21 02:04:06 Downloading - Downloading the training image..................\n",
      "2024-01-21 02:06:52 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34mRunning custom environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:97: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:18 INFO 140171572815680] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:18 INFO 140171572815680] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'context_length': '10', 'epochs': '10', 'likelihood': 'gaussian', 'num_cells': '50', 'num_layers': '2', 'prediction_length': '5', 'time_freq': 'D'}\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:18 INFO 140171572815680] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'gaussian', 'mini_batch_size': '128', 'num_cells': '50', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'context_length': '10', 'epochs': '10', 'prediction_length': '5', 'time_freq': 'D'}\u001b[0m\n",
      "\u001b[34mProcess 7 is a worker.\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:18 INFO 140171572815680] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:18 INFO 140171572815680] random_seed is None\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:18 INFO 140171572815680] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/sales_data.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:18 INFO 140171572815680] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/sales_data.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:24 INFO 140171572815680] Training set statistics:\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:24 INFO 140171572815680] Real time series\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:24 INFO 140171572815680] number of time series: 29715\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:24 INFO 140171572815680] number of observations: 16542969\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:24 INFO 140171572815680] mean target length: 556.7211509338717\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:24 INFO 140171572815680] min/mean/max target: 3.0/95.74755612288227/1499.4000244140625\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:24 INFO 140171572815680] mean abs(target): 95.74755612288227\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:24 INFO 140171572815680] contains missing values: no\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:24 INFO 140171572815680] No test channel found not running evaluations\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/algorithm/core/date_feature_set.py:44: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return index.weekofyear / 51.0 - 0.5\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:24 INFO 140171572815680] #memory_usage::<batchbuffer> = 14.82421875 mb\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:24 INFO 140171572815680] nvidia-smi: took 0.032 seconds to run.\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:24 INFO 140171572815680] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:24 INFO 140171572815680] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:24 INFO 140171572815680] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1705802844.9237761, \"EndTime\": 1705802844.9701257, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 45.224905014038086, \"count\": 1, \"min\": 45.224905014038086, \"max\": 45.224905014038086}}}\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:24 INFO 140171572815680] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:25 INFO 140171572815680] #memory_usage::<model> = 16 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1705802844.9702497, \"EndTime\": 1705802845.0241644, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 100.2650260925293, \"count\": 1, \"min\": 100.2650260925293, \"max\": 100.2650260925293}}}\u001b[0m\n",
      "\u001b[34m[02:07:25] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_Cuda_11.1.x.398.0/AL2_x86_64/generic-flavor/src/src/operator/nn/mkldnn/mkldnn_base.cc:74: Allocate 25600 bytes with malloc directly\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:25 INFO 140171572815680] Epoch[0] Batch[0] avg_epoch_loss=9.227382\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:25 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=9.227381706237793\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:26 INFO 140171572815680] Epoch[0] Batch[5] avg_epoch_loss=7.502662\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:26 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=7.502661784489949\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:26 INFO 140171572815680] Epoch[0] Batch [5]#011Speed: 1791.88 samples/sec#011loss=7.502662\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:26 INFO 140171572815680] Epoch[0] Batch[10] avg_epoch_loss=6.952303\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:26 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=6.291873455047607\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:26 INFO 140171572815680] Epoch[0] Batch [10]#011Speed: 830.98 samples/sec#011loss=6.291873\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:27 INFO 140171572815680] Epoch[0] Batch[15] avg_epoch_loss=6.738573\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:27 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=15 train loss <loss>=6.26836519241333\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:27 INFO 140171572815680] Epoch[0] Batch [15]#011Speed: 1658.86 samples/sec#011loss=6.268365\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:28 INFO 140171572815680] Epoch[0] Batch[20] avg_epoch_loss=6.628273\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:28 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=20 train loss <loss>=6.275314617156982\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:28 INFO 140171572815680] Epoch[0] Batch [20]#011Speed: 672.37 samples/sec#011loss=6.275315\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:28 INFO 140171572815680] Epoch[0] Batch[25] avg_epoch_loss=6.774033\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:28 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=25 train loss <loss>=7.386224269866943\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:28 INFO 140171572815680] Epoch[0] Batch [25]#011Speed: 1406.35 samples/sec#011loss=7.386224\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:29 INFO 140171572815680] Epoch[0] Batch[30] avg_epoch_loss=6.678179\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:29 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=30 train loss <loss>=6.179739284515381\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:29 INFO 140171572815680] Epoch[0] Batch [30]#011Speed: 601.11 samples/sec#011loss=6.179739\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:30 INFO 140171572815680] Epoch[0] Batch[35] avg_epoch_loss=6.608351\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:30 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=35 train loss <loss>=6.175417518615722\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:30 INFO 140171572815680] Epoch[0] Batch [35]#011Speed: 1416.29 samples/sec#011loss=6.175418\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:30 INFO 140171572815680] Epoch[0] Batch[40] avg_epoch_loss=6.561101\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:30 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=40 train loss <loss>=6.2209007263183596\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:30 INFO 140171572815680] Epoch[0] Batch [40]#011Speed: 841.01 samples/sec#011loss=6.220901\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:31 INFO 140171572815680] Epoch[0] Batch[45] avg_epoch_loss=6.518427\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:31 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=45 train loss <loss>=6.168496131896973\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:31 INFO 140171572815680] Epoch[0] Batch [45]#011Speed: 1737.99 samples/sec#011loss=6.168496\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:32 INFO 140171572815680] Epoch[0] Batch[50] avg_epoch_loss=6.482996\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:32 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=50 train loss <loss>=6.157035827636719\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:32 INFO 140171572815680] Epoch[0] Batch [50]#011Speed: 829.96 samples/sec#011loss=6.157036\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:32 INFO 140171572815680] Epoch[0] Batch[55] avg_epoch_loss=6.452387\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:32 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=55 train loss <loss>=6.140170574188232\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:32 INFO 140171572815680] Epoch[0] Batch [55]#011Speed: 1794.75 samples/sec#011loss=6.140171\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:33 INFO 140171572815680] Epoch[0] Batch[60] avg_epoch_loss=6.422756\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:33 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=60 train loss <loss>=6.090888977050781\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:33 INFO 140171572815680] Epoch[0] Batch [60]#011Speed: 824.00 samples/sec#011loss=6.090889\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:33 INFO 140171572815680] Epoch[0] Batch[65] avg_epoch_loss=6.402909\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:33 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=65 train loss <loss>=6.1607842445373535\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:33 INFO 140171572815680] Epoch[0] Batch [65]#011Speed: 1846.82 samples/sec#011loss=6.160784\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:34 INFO 140171572815680] Epoch[0] Batch[70] avg_epoch_loss=6.387699\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:34 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=70 train loss <loss>=6.186927032470703\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:34 INFO 140171572815680] Epoch[0] Batch [70]#011Speed: 832.97 samples/sec#011loss=6.186927\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:34 INFO 140171572815680] Epoch[0] Batch[75] avg_epoch_loss=6.371487\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:34 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=75 train loss <loss>=6.141271877288818\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:34 INFO 140171572815680] Epoch[0] Batch [75]#011Speed: 1866.67 samples/sec#011loss=6.141272\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:35 INFO 140171572815680] Epoch[0] Batch[80] avg_epoch_loss=6.356869\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:35 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=80 train loss <loss>=6.134673023223877\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:35 INFO 140171572815680] Epoch[0] Batch [80]#011Speed: 824.36 samples/sec#011loss=6.134673\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:35 INFO 140171572815680] Epoch[0] Batch[85] avg_epoch_loss=6.347570\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:35 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=85 train loss <loss>=6.196930885314941\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:35 INFO 140171572815680] Epoch[0] Batch [85]#011Speed: 1754.00 samples/sec#011loss=6.196931\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:36 INFO 140171572815680] Epoch[0] Batch[90] avg_epoch_loss=6.334453\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:36 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=90 train loss <loss>=6.108831596374512\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:36 INFO 140171572815680] Epoch[0] Batch [90]#011Speed: 812.44 samples/sec#011loss=6.108832\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:36 INFO 140171572815680] Epoch[0] Batch[95] avg_epoch_loss=6.321602\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:36 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=95 train loss <loss>=6.087713432312012\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:36 INFO 140171572815680] Epoch[0] Batch [95]#011Speed: 1870.37 samples/sec#011loss=6.087713\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:37 INFO 140171572815680] Epoch[0] Batch[100] avg_epoch_loss=6.310949\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:37 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=100 train loss <loss>=6.106409931182862\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:37 INFO 140171572815680] Epoch[0] Batch [100]#011Speed: 838.22 samples/sec#011loss=6.106410\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:38 INFO 140171572815680] Epoch[0] Batch[105] avg_epoch_loss=6.302446\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:38 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=105 train loss <loss>=6.130691814422607\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:38 INFO 140171572815680] Epoch[0] Batch [105]#011Speed: 1701.17 samples/sec#011loss=6.130692\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:38 INFO 140171572815680] Epoch[0] Batch[110] avg_epoch_loss=6.293781\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:38 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=110 train loss <loss>=6.110092735290527\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:38 INFO 140171572815680] Epoch[0] Batch [110]#011Speed: 833.29 samples/sec#011loss=6.110093\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:39 INFO 140171572815680] Epoch[0] Batch[115] avg_epoch_loss=6.285814\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:39 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=115 train loss <loss>=6.108928108215332\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:39 INFO 140171572815680] Epoch[0] Batch [115]#011Speed: 1777.60 samples/sec#011loss=6.108928\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:39 INFO 140171572815680] Epoch[0] Batch[120] avg_epoch_loss=6.279279\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:39 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=120 train loss <loss>=6.127667236328125\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:39 INFO 140171572815680] Epoch[0] Batch [120]#011Speed: 823.21 samples/sec#011loss=6.127667\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:40 INFO 140171572815680] Epoch[0] Batch[125] avg_epoch_loss=6.273865\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:40 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=125 train loss <loss>=6.1428656578063965\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:40 INFO 140171572815680] Epoch[0] Batch [125]#011Speed: 1875.33 samples/sec#011loss=6.142866\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:41 INFO 140171572815680] Epoch[0] Batch[130] avg_epoch_loss=6.266747\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:41 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=130 train loss <loss>=6.0873723983764645\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:41 INFO 140171572815680] Epoch[0] Batch [130]#011Speed: 845.02 samples/sec#011loss=6.087372\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:41 INFO 140171572815680] Epoch[0] Batch[135] avg_epoch_loss=6.260849\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:41 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=135 train loss <loss>=6.1063072204589846\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:41 INFO 140171572815680] Epoch[0] Batch [135]#011Speed: 1833.88 samples/sec#011loss=6.106307\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:42 INFO 140171572815680] Epoch[0] Batch[140] avg_epoch_loss=6.254103\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:42 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=140 train loss <loss>=6.070609188079834\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:42 INFO 140171572815680] Epoch[0] Batch [140]#011Speed: 804.91 samples/sec#011loss=6.070609\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:42 INFO 140171572815680] Epoch[0] Batch[145] avg_epoch_loss=6.248586\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:42 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=145 train loss <loss>=6.093018913269043\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:42 INFO 140171572815680] Epoch[0] Batch [145]#011Speed: 1717.01 samples/sec#011loss=6.093019\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:43 INFO 140171572815680] Epoch[0] Batch[150] avg_epoch_loss=6.242179\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:43 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=150 train loss <loss>=6.055074691772461\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:43 INFO 140171572815680] Epoch[0] Batch [150]#011Speed: 831.80 samples/sec#011loss=6.055075\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:43 INFO 140171572815680] Epoch[0] Batch[155] avg_epoch_loss=6.237049\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:43 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=155 train loss <loss>=6.082124805450439\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:43 INFO 140171572815680] Epoch[0] Batch [155]#011Speed: 1851.98 samples/sec#011loss=6.082125\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:44 INFO 140171572815680] Epoch[0] Batch[160] avg_epoch_loss=6.231143\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:44 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=160 train loss <loss>=6.046883010864258\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:44 INFO 140171572815680] Epoch[0] Batch [160]#011Speed: 836.31 samples/sec#011loss=6.046883\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:44 INFO 140171572815680] Epoch[0] Batch[165] avg_epoch_loss=6.225446\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:44 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=165 train loss <loss>=6.042019462585449\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:44 INFO 140171572815680] Epoch[0] Batch [165]#011Speed: 1870.38 samples/sec#011loss=6.042019\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:45 INFO 140171572815680] Epoch[0] Batch[170] avg_epoch_loss=6.221751\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:45 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=170 train loss <loss>=6.099071979522705\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:45 INFO 140171572815680] Epoch[0] Batch [170]#011Speed: 824.70 samples/sec#011loss=6.099072\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:45 INFO 140171572815680] Epoch[0] Batch[175] avg_epoch_loss=6.218411\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:45 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=175 train loss <loss>=6.104171848297119\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:45 INFO 140171572815680] Epoch[0] Batch [175]#011Speed: 1821.50 samples/sec#011loss=6.104172\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:46 INFO 140171572815680] Epoch[0] Batch[180] avg_epoch_loss=6.214904\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:46 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=180 train loss <loss>=6.091467189788818\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:46 INFO 140171572815680] Epoch[0] Batch [180]#011Speed: 819.35 samples/sec#011loss=6.091467\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:47 INFO 140171572815680] Epoch[0] Batch[185] avg_epoch_loss=6.211107\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:47 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=185 train loss <loss>=6.0736565589904785\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:47 INFO 140171572815680] Epoch[0] Batch [185]#011Speed: 1814.45 samples/sec#011loss=6.073657\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:47 INFO 140171572815680] Epoch[0] Batch[190] avg_epoch_loss=6.206841\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:47 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=190 train loss <loss>=6.048148536682129\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:47 INFO 140171572815680] Epoch[0] Batch [190]#011Speed: 842.73 samples/sec#011loss=6.048149\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:48 INFO 140171572815680] Epoch[0] Batch[195] avg_epoch_loss=6.202507\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:48 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=195 train loss <loss>=6.036953735351562\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:48 INFO 140171572815680] Epoch[0] Batch [195]#011Speed: 1842.14 samples/sec#011loss=6.036954\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:48 INFO 140171572815680] Epoch[0] Batch[200] avg_epoch_loss=6.198838\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:48 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=200 train loss <loss>=6.054990673065186\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:48 INFO 140171572815680] Epoch[0] Batch [200]#011Speed: 840.15 samples/sec#011loss=6.054991\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:49 INFO 140171572815680] Epoch[0] Batch[205] avg_epoch_loss=6.194866\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:49 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=205 train loss <loss>=6.035190010070801\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:49 INFO 140171572815680] Epoch[0] Batch [205]#011Speed: 1880.09 samples/sec#011loss=6.035190\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:50 INFO 140171572815680] Epoch[0] Batch[210] avg_epoch_loss=6.190926\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:50 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=210 train loss <loss>=6.028610038757324\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:50 INFO 140171572815680] Epoch[0] Batch [210]#011Speed: 819.94 samples/sec#011loss=6.028610\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:50 INFO 140171572815680] Epoch[0] Batch[215] avg_epoch_loss=6.186267\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:50 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=215 train loss <loss>=5.989670944213867\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:50 INFO 140171572815680] Epoch[0] Batch [215]#011Speed: 1870.69 samples/sec#011loss=5.989671\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:51 INFO 140171572815680] Epoch[0] Batch[220] avg_epoch_loss=6.182503\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:51 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=220 train loss <loss>=6.019874191284179\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:51 INFO 140171572815680] Epoch[0] Batch [220]#011Speed: 836.63 samples/sec#011loss=6.019874\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:51 INFO 140171572815680] Epoch[0] Batch[225] avg_epoch_loss=6.178410\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:51 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=225 train loss <loss>=5.997514247894287\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:51 INFO 140171572815680] Epoch[0] Batch [225]#011Speed: 1841.57 samples/sec#011loss=5.997514\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:51 INFO 140171572815680] Epoch[0] Batch[230] avg_epoch_loss=6.174669\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:51 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, batch=230 train loss <loss>=6.005563735961914\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:51 INFO 140171572815680] Epoch[0] Batch [230]#011Speed: 1210.17 samples/sec#011loss=6.005564\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:52 INFO 140171572815680] processed a total of 29897 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1705802845.024245, \"EndTime\": 1705802872.224238, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"update.time\": {\"sum\": 27199.859619140625, \"count\": 1, \"min\": 27199.859619140625, \"max\": 27199.859619140625}}}\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:52 INFO 140171572815680] #throughput_metric: host=algo-1, train throughput=1099.1537450566086 records/second\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:52 INFO 140171572815680] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:52 INFO 140171572815680] #quality_metric: host=algo-1, epoch=0, train loss <loss>=6.171213090929211\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:52 INFO 140171572815680] Epoch[1] Batch[0] avg_epoch_loss=6.061281\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:52 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=6.061281204223633\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:53 INFO 140171572815680] Epoch[1] Batch[5] avg_epoch_loss=6.077368\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:53 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=6.077368259429932\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:53 INFO 140171572815680] Epoch[1] Batch [5]#011Speed: 1819.00 samples/sec#011loss=6.077368\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:53 INFO 140171572815680] Epoch[1] Batch[10] avg_epoch_loss=6.056529\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:53 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=6.031521797180176\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:53 INFO 140171572815680] Epoch[1] Batch [10]#011Speed: 816.94 samples/sec#011loss=6.031522\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:54 INFO 140171572815680] Epoch[1] Batch[15] avg_epoch_loss=6.057999\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:54 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=15 train loss <loss>=6.061233615875244\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:54 INFO 140171572815680] Epoch[1] Batch [15]#011Speed: 1824.30 samples/sec#011loss=6.061234\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:54 INFO 140171572815680] Epoch[1] Batch[20] avg_epoch_loss=6.040873\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:54 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=20 train loss <loss>=5.986069297790527\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:54 INFO 140171572815680] Epoch[1] Batch [20]#011Speed: 821.93 samples/sec#011loss=5.986069\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:55 INFO 140171572815680] Epoch[1] Batch[25] avg_epoch_loss=6.027221\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:55 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=25 train loss <loss>=5.969883632659912\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:55 INFO 140171572815680] Epoch[1] Batch [25]#011Speed: 1849.27 samples/sec#011loss=5.969884\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:56 INFO 140171572815680] Epoch[1] Batch[30] avg_epoch_loss=6.016752\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:56 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=30 train loss <loss>=5.962312698364258\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:56 INFO 140171572815680] Epoch[1] Batch [30]#011Speed: 834.93 samples/sec#011loss=5.962313\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:56 INFO 140171572815680] Epoch[1] Batch[35] avg_epoch_loss=6.011672\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:56 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=35 train loss <loss>=5.980175495147705\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:56 INFO 140171572815680] Epoch[1] Batch [35]#011Speed: 1566.20 samples/sec#011loss=5.980175\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:57 INFO 140171572815680] Epoch[1] Batch[40] avg_epoch_loss=6.011111\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:57 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=40 train loss <loss>=6.007068538665772\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:57 INFO 140171572815680] Epoch[1] Batch [40]#011Speed: 707.34 samples/sec#011loss=6.007069\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:57 INFO 140171572815680] Epoch[1] Batch[45] avg_epoch_loss=6.007908\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:57 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=45 train loss <loss>=5.981650257110596\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:57 INFO 140171572815680] Epoch[1] Batch [45]#011Speed: 1271.97 samples/sec#011loss=5.981650\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:58 INFO 140171572815680] Epoch[1] Batch[50] avg_epoch_loss=6.075027\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:58 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=50 train loss <loss>=6.692514324188233\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:58 INFO 140171572815680] Epoch[1] Batch [50]#011Speed: 804.50 samples/sec#011loss=6.692514\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:59 INFO 140171572815680] Epoch[1] Batch[55] avg_epoch_loss=6.072498\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:59 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=55 train loss <loss>=6.046710681915283\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:59 INFO 140171572815680] Epoch[1] Batch [55]#011Speed: 1850.73 samples/sec#011loss=6.046711\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:59 INFO 140171572815680] Epoch[1] Batch[60] avg_epoch_loss=6.067255\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:59 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=60 train loss <loss>=6.008527851104736\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:07:59 INFO 140171572815680] Epoch[1] Batch [60]#011Speed: 841.92 samples/sec#011loss=6.008528\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:00 INFO 140171572815680] Epoch[1] Batch[65] avg_epoch_loss=6.062042\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:00 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=65 train loss <loss>=5.99844970703125\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:00 INFO 140171572815680] Epoch[1] Batch [65]#011Speed: 1632.70 samples/sec#011loss=5.998450\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:01 INFO 140171572815680] Epoch[1] Batch[70] avg_epoch_loss=6.059535\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:01 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=70 train loss <loss>=6.02644100189209\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:01 INFO 140171572815680] Epoch[1] Batch [70]#011Speed: 707.02 samples/sec#011loss=6.026441\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:01 INFO 140171572815680] Epoch[1] Batch[75] avg_epoch_loss=6.053843\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:01 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=75 train loss <loss>=5.973016834259033\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:01 INFO 140171572815680] Epoch[1] Batch [75]#011Speed: 1786.78 samples/sec#011loss=5.973017\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:02 INFO 140171572815680] Epoch[1] Batch[80] avg_epoch_loss=6.050080\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:02 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=80 train loss <loss>=5.992875385284424\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:02 INFO 140171572815680] Epoch[1] Batch [80]#011Speed: 733.97 samples/sec#011loss=5.992875\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:02 INFO 140171572815680] Epoch[1] Batch[85] avg_epoch_loss=6.045780\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:02 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=85 train loss <loss>=5.976117324829102\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:02 INFO 140171572815680] Epoch[1] Batch [85]#011Speed: 1540.54 samples/sec#011loss=5.976117\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:03 INFO 140171572815680] Epoch[1] Batch[90] avg_epoch_loss=6.045849\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:03 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=90 train loss <loss>=6.047047805786133\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:03 INFO 140171572815680] Epoch[1] Batch [90]#011Speed: 606.70 samples/sec#011loss=6.047048\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:04 INFO 140171572815680] Epoch[1] Batch[95] avg_epoch_loss=6.041793\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:04 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=95 train loss <loss>=5.967972660064698\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:04 INFO 140171572815680] Epoch[1] Batch [95]#011Speed: 1829.39 samples/sec#011loss=5.967973\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:04 INFO 140171572815680] Epoch[1] Batch[100] avg_epoch_loss=6.040073\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:04 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=100 train loss <loss>=6.007034015655518\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:04 INFO 140171572815680] Epoch[1] Batch [100]#011Speed: 837.72 samples/sec#011loss=6.007034\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:05 INFO 140171572815680] Epoch[1] Batch[105] avg_epoch_loss=6.038261\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:05 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=105 train loss <loss>=6.001666927337647\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:05 INFO 140171572815680] Epoch[1] Batch [105]#011Speed: 1789.95 samples/sec#011loss=6.001667\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:06 INFO 140171572815680] Epoch[1] Batch[110] avg_epoch_loss=6.038031\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:06 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=110 train loss <loss>=6.0331549644470215\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:06 INFO 140171572815680] Epoch[1] Batch [110]#011Speed: 821.25 samples/sec#011loss=6.033155\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:06 INFO 140171572815680] Epoch[1] Batch[115] avg_epoch_loss=6.037740\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:06 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=115 train loss <loss>=6.031287002563476\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:06 INFO 140171572815680] Epoch[1] Batch [115]#011Speed: 1807.78 samples/sec#011loss=6.031287\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:07 INFO 140171572815680] Epoch[1] Batch[120] avg_epoch_loss=6.035691\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:07 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=120 train loss <loss>=5.988155746459961\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:07 INFO 140171572815680] Epoch[1] Batch [120]#011Speed: 816.25 samples/sec#011loss=5.988156\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:07 INFO 140171572815680] Epoch[1] Batch[125] avg_epoch_loss=6.033683\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:07 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=125 train loss <loss>=5.985074234008789\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:07 INFO 140171572815680] Epoch[1] Batch [125]#011Speed: 1794.44 samples/sec#011loss=5.985074\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:08 INFO 140171572815680] Epoch[1] Batch[130] avg_epoch_loss=6.030521\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:08 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=130 train loss <loss>=5.950852489471435\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:08 INFO 140171572815680] Epoch[1] Batch [130]#011Speed: 811.30 samples/sec#011loss=5.950852\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:08 INFO 140171572815680] Epoch[1] Batch[135] avg_epoch_loss=6.032732\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:08 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=135 train loss <loss>=6.090642833709717\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:08 INFO 140171572815680] Epoch[1] Batch [135]#011Speed: 1753.58 samples/sec#011loss=6.090643\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:09 INFO 140171572815680] Epoch[1] Batch[140] avg_epoch_loss=6.031683\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:09 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=140 train loss <loss>=6.003170585632324\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:09 INFO 140171572815680] Epoch[1] Batch [140]#011Speed: 806.69 samples/sec#011loss=6.003171\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:09 INFO 140171572815680] Epoch[1] Batch[145] avg_epoch_loss=6.029961\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:09 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=145 train loss <loss>=5.981397819519043\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:09 INFO 140171572815680] Epoch[1] Batch [145]#011Speed: 1589.28 samples/sec#011loss=5.981398\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:10 INFO 140171572815680] Epoch[1] Batch[150] avg_epoch_loss=6.027952\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:10 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=150 train loss <loss>=5.96926794052124\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:10 INFO 140171572815680] Epoch[1] Batch [150]#011Speed: 787.73 samples/sec#011loss=5.969268\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:11 INFO 140171572815680] Epoch[1] Batch[155] avg_epoch_loss=6.027267\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:11 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=155 train loss <loss>=6.006595611572266\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:11 INFO 140171572815680] Epoch[1] Batch [155]#011Speed: 1821.40 samples/sec#011loss=6.006596\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:11 INFO 140171572815680] Epoch[1] Batch[160] avg_epoch_loss=6.025397\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:11 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=160 train loss <loss>=5.967046642303467\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:11 INFO 140171572815680] Epoch[1] Batch [160]#011Speed: 800.25 samples/sec#011loss=5.967047\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:12 INFO 140171572815680] Epoch[1] Batch[165] avg_epoch_loss=6.023764\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:12 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=165 train loss <loss>=5.971187496185303\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:12 INFO 140171572815680] Epoch[1] Batch [165]#011Speed: 1756.61 samples/sec#011loss=5.971187\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:13 INFO 140171572815680] Epoch[1] Batch[170] avg_epoch_loss=6.021623\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:13 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=170 train loss <loss>=5.950545787811279\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:13 INFO 140171572815680] Epoch[1] Batch [170]#011Speed: 815.19 samples/sec#011loss=5.950546\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:13 INFO 140171572815680] Epoch[1] Batch[175] avg_epoch_loss=6.020139\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:13 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=175 train loss <loss>=5.9693902969360355\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:13 INFO 140171572815680] Epoch[1] Batch [175]#011Speed: 1820.04 samples/sec#011loss=5.969390\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:14 INFO 140171572815680] Epoch[1] Batch[180] avg_epoch_loss=6.018701\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:14 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=180 train loss <loss>=5.968061065673828\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:14 INFO 140171572815680] Epoch[1] Batch [180]#011Speed: 848.71 samples/sec#011loss=5.968061\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:14 INFO 140171572815680] Epoch[1] Batch[185] avg_epoch_loss=6.017047\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:14 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=185 train loss <loss>=5.957167720794677\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:14 INFO 140171572815680] Epoch[1] Batch [185]#011Speed: 1789.60 samples/sec#011loss=5.957168\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:15 INFO 140171572815680] Epoch[1] Batch[190] avg_epoch_loss=6.013761\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:15 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=190 train loss <loss>=5.891552352905274\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:15 INFO 140171572815680] Epoch[1] Batch [190]#011Speed: 837.01 samples/sec#011loss=5.891552\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:15 INFO 140171572815680] Epoch[1] Batch[195] avg_epoch_loss=6.011330\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:15 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=195 train loss <loss>=5.918471050262451\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:15 INFO 140171572815680] Epoch[1] Batch [195]#011Speed: 1829.61 samples/sec#011loss=5.918471\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:16 INFO 140171572815680] Epoch[1] Batch[200] avg_epoch_loss=6.009762\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:16 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=200 train loss <loss>=5.948274803161621\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:16 INFO 140171572815680] Epoch[1] Batch [200]#011Speed: 829.02 samples/sec#011loss=5.948275\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:16 INFO 140171572815680] Epoch[1] Batch[205] avg_epoch_loss=6.007953\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:16 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=205 train loss <loss>=5.935254859924316\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:16 INFO 140171572815680] Epoch[1] Batch [205]#011Speed: 1650.23 samples/sec#011loss=5.935255\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:17 INFO 140171572815680] Epoch[1] Batch[210] avg_epoch_loss=6.009716\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:17 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=210 train loss <loss>=6.082351016998291\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:17 INFO 140171572815680] Epoch[1] Batch [210]#011Speed: 842.33 samples/sec#011loss=6.082351\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:17 INFO 140171572815680] Epoch[1] Batch[215] avg_epoch_loss=6.009870\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:17 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=215 train loss <loss>=6.01633586883545\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:17 INFO 140171572815680] Epoch[1] Batch [215]#011Speed: 1888.01 samples/sec#011loss=6.016336\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:18 INFO 140171572815680] Epoch[1] Batch[220] avg_epoch_loss=6.027129\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:18 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=220 train loss <loss>=6.772723007202148\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:18 INFO 140171572815680] Epoch[1] Batch [220]#011Speed: 823.74 samples/sec#011loss=6.772723\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:18 INFO 140171572815680] Epoch[1] Batch[225] avg_epoch_loss=6.026303\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:18 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, batch=225 train loss <loss>=5.989820194244385\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:18 INFO 140171572815680] Epoch[1] Batch [225]#011Speed: 1878.91 samples/sec#011loss=5.989820\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:19 INFO 140171572815680] processed a total of 29393 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1705802872.2243335, \"EndTime\": 1705802899.2607625, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 27035.66575050354, \"count\": 1, \"min\": 27035.66575050354, \"max\": 27035.66575050354}}}\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:19 INFO 140171572815680] #throughput_metric: host=algo-1, train throughput=1087.1873708910086 records/second\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:19 INFO 140171572815680] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:19 INFO 140171572815680] #quality_metric: host=algo-1, epoch=1, train loss <loss>=6.025166055430537\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:19 INFO 140171572815680] Epoch[2] Batch[0] avg_epoch_loss=5.983809\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:19 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=5.983808994293213\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:20 INFO 140171572815680] Epoch[2] Batch[5] avg_epoch_loss=6.013170\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:20 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=6.01317032178243\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:20 INFO 140171572815680] Epoch[2] Batch [5]#011Speed: 1642.53 samples/sec#011loss=6.013170\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:20 INFO 140171572815680] Epoch[2] Batch[10] avg_epoch_loss=5.995911\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:20 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=5.97519998550415\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:20 INFO 140171572815680] Epoch[2] Batch [10]#011Speed: 804.06 samples/sec#011loss=5.975200\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:21 INFO 140171572815680] Epoch[2] Batch[15] avg_epoch_loss=5.981169\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:21 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=15 train loss <loss>=5.948734951019287\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:21 INFO 140171572815680] Epoch[2] Batch [15]#011Speed: 1909.80 samples/sec#011loss=5.948735\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:22 INFO 140171572815680] Epoch[2] Batch[20] avg_epoch_loss=5.980450\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:22 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=20 train loss <loss>=5.978151893615722\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:22 INFO 140171572815680] Epoch[2] Batch [20]#011Speed: 828.06 samples/sec#011loss=5.978152\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:22 INFO 140171572815680] Epoch[2] Batch[25] avg_epoch_loss=5.978169\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:22 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=25 train loss <loss>=5.96858777999878\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:22 INFO 140171572815680] Epoch[2] Batch [25]#011Speed: 1798.90 samples/sec#011loss=5.968588\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:23 INFO 140171572815680] Epoch[2] Batch[30] avg_epoch_loss=5.978583\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:23 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=30 train loss <loss>=5.980734729766846\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:23 INFO 140171572815680] Epoch[2] Batch [30]#011Speed: 829.27 samples/sec#011loss=5.980735\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:23 INFO 140171572815680] Epoch[2] Batch[35] avg_epoch_loss=5.981552\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:23 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=35 train loss <loss>=5.9999617576599125\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:23 INFO 140171572815680] Epoch[2] Batch [35]#011Speed: 1815.74 samples/sec#011loss=5.999962\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:24 INFO 140171572815680] Epoch[2] Batch[40] avg_epoch_loss=5.981940\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:24 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=40 train loss <loss>=5.984729290008545\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:24 INFO 140171572815680] Epoch[2] Batch [40]#011Speed: 808.82 samples/sec#011loss=5.984729\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:24 INFO 140171572815680] Epoch[2] Batch[45] avg_epoch_loss=5.976870\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:24 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=45 train loss <loss>=5.935296535491943\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:24 INFO 140171572815680] Epoch[2] Batch [45]#011Speed: 1831.46 samples/sec#011loss=5.935297\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:25 INFO 140171572815680] Epoch[2] Batch[50] avg_epoch_loss=5.977146\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:25 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=50 train loss <loss>=5.979692459106445\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:25 INFO 140171572815680] Epoch[2] Batch [50]#011Speed: 819.11 samples/sec#011loss=5.979692\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:25 INFO 140171572815680] Epoch[2] Batch[55] avg_epoch_loss=5.976313\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:25 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=55 train loss <loss>=5.967811870574951\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:25 INFO 140171572815680] Epoch[2] Batch [55]#011Speed: 1834.76 samples/sec#011loss=5.967812\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:26 INFO 140171572815680] Epoch[2] Batch[60] avg_epoch_loss=5.976734\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:26 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=60 train loss <loss>=5.9814458847045895\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:26 INFO 140171572815680] Epoch[2] Batch [60]#011Speed: 823.39 samples/sec#011loss=5.981446\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:26 INFO 140171572815680] Epoch[2] Batch[65] avg_epoch_loss=5.976194\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:26 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=65 train loss <loss>=5.969615650177002\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:26 INFO 140171572815680] Epoch[2] Batch [65]#011Speed: 1885.07 samples/sec#011loss=5.969616\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:27 INFO 140171572815680] Epoch[2] Batch[70] avg_epoch_loss=5.974311\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:27 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=70 train loss <loss>=5.949453639984131\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:27 INFO 140171572815680] Epoch[2] Batch [70]#011Speed: 838.87 samples/sec#011loss=5.949454\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:28 INFO 140171572815680] Epoch[2] Batch[75] avg_epoch_loss=5.975449\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:28 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=75 train loss <loss>=5.991597270965576\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:28 INFO 140171572815680] Epoch[2] Batch [75]#011Speed: 1893.69 samples/sec#011loss=5.991597\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:28 INFO 140171572815680] Epoch[2] Batch[80] avg_epoch_loss=5.975383\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:28 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=80 train loss <loss>=5.974391174316406\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:28 INFO 140171572815680] Epoch[2] Batch [80]#011Speed: 833.80 samples/sec#011loss=5.974391\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:29 INFO 140171572815680] Epoch[2] Batch[85] avg_epoch_loss=5.974689\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:29 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=85 train loss <loss>=5.963447570800781\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:29 INFO 140171572815680] Epoch[2] Batch [85]#011Speed: 1889.59 samples/sec#011loss=5.963448\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:29 INFO 140171572815680] Epoch[2] Batch[90] avg_epoch_loss=5.977294\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:29 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=90 train loss <loss>=6.022087860107422\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:29 INFO 140171572815680] Epoch[2] Batch [90]#011Speed: 841.94 samples/sec#011loss=6.022088\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:30 INFO 140171572815680] Epoch[2] Batch[95] avg_epoch_loss=5.975232\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:30 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=95 train loss <loss>=5.937709617614746\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:30 INFO 140171572815680] Epoch[2] Batch [95]#011Speed: 1852.42 samples/sec#011loss=5.937710\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:31 INFO 140171572815680] Epoch[2] Batch[100] avg_epoch_loss=5.973345\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:31 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=100 train loss <loss>=5.9371185302734375\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:31 INFO 140171572815680] Epoch[2] Batch [100]#011Speed: 822.22 samples/sec#011loss=5.937119\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:31 INFO 140171572815680] Epoch[2] Batch[105] avg_epoch_loss=5.971313\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:31 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=105 train loss <loss>=5.930263996124268\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:31 INFO 140171572815680] Epoch[2] Batch [105]#011Speed: 1834.75 samples/sec#011loss=5.930264\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:32 INFO 140171572815680] Epoch[2] Batch[110] avg_epoch_loss=5.973144\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:32 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=110 train loss <loss>=6.011949348449707\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:32 INFO 140171572815680] Epoch[2] Batch [110]#011Speed: 786.36 samples/sec#011loss=6.011949\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:32 INFO 140171572815680] Epoch[2] Batch[115] avg_epoch_loss=5.971267\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:32 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=115 train loss <loss>=5.929599761962891\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:32 INFO 140171572815680] Epoch[2] Batch [115]#011Speed: 1797.15 samples/sec#011loss=5.929600\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:33 INFO 140171572815680] Epoch[2] Batch[120] avg_epoch_loss=5.969328\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:33 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=120 train loss <loss>=5.924360942840576\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:33 INFO 140171572815680] Epoch[2] Batch [120]#011Speed: 825.14 samples/sec#011loss=5.924361\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:33 INFO 140171572815680] Epoch[2] Batch[125] avg_epoch_loss=5.968365\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:33 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=125 train loss <loss>=5.945057678222656\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:33 INFO 140171572815680] Epoch[2] Batch [125]#011Speed: 1707.44 samples/sec#011loss=5.945058\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:34 INFO 140171572815680] Epoch[2] Batch[130] avg_epoch_loss=5.967518\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:34 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=130 train loss <loss>=5.946176624298095\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:34 INFO 140171572815680] Epoch[2] Batch [130]#011Speed: 841.99 samples/sec#011loss=5.946177\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:34 INFO 140171572815680] Epoch[2] Batch[135] avg_epoch_loss=5.967323\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:34 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=135 train loss <loss>=5.96219835281372\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:34 INFO 140171572815680] Epoch[2] Batch [135]#011Speed: 1692.21 samples/sec#011loss=5.962198\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:35 INFO 140171572815680] Epoch[2] Batch[140] avg_epoch_loss=5.967186\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:35 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=140 train loss <loss>=5.963469505310059\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:35 INFO 140171572815680] Epoch[2] Batch [140]#011Speed: 840.82 samples/sec#011loss=5.963470\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:35 INFO 140171572815680] Epoch[2] Batch[145] avg_epoch_loss=5.965596\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:35 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=145 train loss <loss>=5.920753383636475\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:35 INFO 140171572815680] Epoch[2] Batch [145]#011Speed: 1709.66 samples/sec#011loss=5.920753\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:36 INFO 140171572815680] Epoch[2] Batch[150] avg_epoch_loss=5.983983\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:36 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=150 train loss <loss>=6.520887947082519\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:36 INFO 140171572815680] Epoch[2] Batch [150]#011Speed: 815.91 samples/sec#011loss=6.520888\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:37 INFO 140171572815680] Epoch[2] Batch[155] avg_epoch_loss=5.984075\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:37 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=155 train loss <loss>=5.986840438842774\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:37 INFO 140171572815680] Epoch[2] Batch [155]#011Speed: 1707.53 samples/sec#011loss=5.986840\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:37 INFO 140171572815680] Epoch[2] Batch[160] avg_epoch_loss=5.985312\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:37 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=160 train loss <loss>=6.023900318145752\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:37 INFO 140171572815680] Epoch[2] Batch [160]#011Speed: 788.91 samples/sec#011loss=6.023900\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:38 INFO 140171572815680] Epoch[2] Batch[165] avg_epoch_loss=5.983609\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:38 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=165 train loss <loss>=5.928791046142578\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:38 INFO 140171572815680] Epoch[2] Batch [165]#011Speed: 1847.57 samples/sec#011loss=5.928791\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:39 INFO 140171572815680] Epoch[2] Batch[170] avg_epoch_loss=5.983923\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:39 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=170 train loss <loss>=5.99434003829956\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:39 INFO 140171572815680] Epoch[2] Batch [170]#011Speed: 819.67 samples/sec#011loss=5.994340\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:39 INFO 140171572815680] Epoch[2] Batch[175] avg_epoch_loss=5.984168\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:39 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=175 train loss <loss>=5.992534828186035\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:39 INFO 140171572815680] Epoch[2] Batch [175]#011Speed: 1887.76 samples/sec#011loss=5.992535\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:40 INFO 140171572815680] Epoch[2] Batch[180] avg_epoch_loss=5.983880\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:40 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=180 train loss <loss>=5.97376766204834\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:40 INFO 140171572815680] Epoch[2] Batch [180]#011Speed: 850.27 samples/sec#011loss=5.973768\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:40 INFO 140171572815680] Epoch[2] Batch[185] avg_epoch_loss=5.982700\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:40 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=185 train loss <loss>=5.939968776702881\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:40 INFO 140171572815680] Epoch[2] Batch [185]#011Speed: 1749.83 samples/sec#011loss=5.939969\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:41 INFO 140171572815680] Epoch[2] Batch[190] avg_epoch_loss=5.981034\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:41 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=190 train loss <loss>=5.919065761566162\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:41 INFO 140171572815680] Epoch[2] Batch [190]#011Speed: 820.03 samples/sec#011loss=5.919066\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:41 INFO 140171572815680] Epoch[2] Batch[195] avg_epoch_loss=5.980098\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:41 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=195 train loss <loss>=5.9443464279174805\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:41 INFO 140171572815680] Epoch[2] Batch [195]#011Speed: 1664.19 samples/sec#011loss=5.944346\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:42 INFO 140171572815680] Epoch[2] Batch[200] avg_epoch_loss=5.979737\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:42 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=200 train loss <loss>=5.965591907501221\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:42 INFO 140171572815680] Epoch[2] Batch [200]#011Speed: 767.00 samples/sec#011loss=5.965592\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:42 INFO 140171572815680] Epoch[2] Batch[205] avg_epoch_loss=5.979704\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:42 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=205 train loss <loss>=5.978360843658447\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:42 INFO 140171572815680] Epoch[2] Batch [205]#011Speed: 1652.21 samples/sec#011loss=5.978361\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:43 INFO 140171572815680] Epoch[2] Batch[210] avg_epoch_loss=5.977700\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:43 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=210 train loss <loss>=5.895149230957031\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:43 INFO 140171572815680] Epoch[2] Batch [210]#011Speed: 849.50 samples/sec#011loss=5.895149\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:44 INFO 140171572815680] Epoch[2] Batch[215] avg_epoch_loss=5.977600\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:44 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=215 train loss <loss>=5.973373031616211\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:44 INFO 140171572815680] Epoch[2] Batch [215]#011Speed: 1830.53 samples/sec#011loss=5.973373\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:44 INFO 140171572815680] Epoch[2] Batch[220] avg_epoch_loss=5.976585\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:44 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=220 train loss <loss>=5.932752227783203\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:44 INFO 140171572815680] Epoch[2] Batch [220]#011Speed: 826.91 samples/sec#011loss=5.932752\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:45 INFO 140171572815680] Epoch[2] Batch[225] avg_epoch_loss=5.975217\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:45 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=225 train loss <loss>=5.914756965637207\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:45 INFO 140171572815680] Epoch[2] Batch [225]#011Speed: 1865.48 samples/sec#011loss=5.914757\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:45 INFO 140171572815680] Epoch[2] Batch[230] avg_epoch_loss=5.974385\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:45 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, batch=230 train loss <loss>=5.936751747131348\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:45 INFO 140171572815680] Epoch[2] Batch [230]#011Speed: 1771.16 samples/sec#011loss=5.936752\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:45 INFO 140171572815680] processed a total of 29443 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1705802899.260864, \"EndTime\": 1705802925.48876, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 26227.423429489136, \"count\": 1, \"min\": 26227.423429489136, \"max\": 26227.423429489136}}}\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:45 INFO 140171572815680] #throughput_metric: host=algo-1, train throughput=1122.5959274293684 records/second\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:45 INFO 140171572815680] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:45 INFO 140171572815680] #quality_metric: host=algo-1, epoch=2, train loss <loss>=5.974384883781532\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:46 INFO 140171572815680] Epoch[3] Batch[0] avg_epoch_loss=5.999815\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:46 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=5.999814987182617\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:46 INFO 140171572815680] Epoch[3] Batch[5] avg_epoch_loss=5.923010\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:46 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=5.9230101108551025\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:46 INFO 140171572815680] Epoch[3] Batch [5]#011Speed: 1762.64 samples/sec#011loss=5.923010\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:47 INFO 140171572815680] Epoch[3] Batch[10] avg_epoch_loss=5.910045\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:47 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=5.894487476348877\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:47 INFO 140171572815680] Epoch[3] Batch [10]#011Speed: 847.26 samples/sec#011loss=5.894487\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:47 INFO 140171572815680] Epoch[3] Batch[15] avg_epoch_loss=5.921809\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:47 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=15 train loss <loss>=5.947689628601074\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:47 INFO 140171572815680] Epoch[3] Batch [15]#011Speed: 1847.81 samples/sec#011loss=5.947690\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:48 INFO 140171572815680] Epoch[3] Batch[20] avg_epoch_loss=5.924699\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:48 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=20 train loss <loss>=5.933946990966797\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:48 INFO 140171572815680] Epoch[3] Batch [20]#011Speed: 821.18 samples/sec#011loss=5.933947\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:48 INFO 140171572815680] Epoch[3] Batch[25] avg_epoch_loss=5.932311\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:48 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=25 train loss <loss>=5.964282894134522\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:48 INFO 140171572815680] Epoch[3] Batch [25]#011Speed: 1820.86 samples/sec#011loss=5.964283\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:49 INFO 140171572815680] Epoch[3] Batch[30] avg_epoch_loss=5.929395\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:49 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=30 train loss <loss>=5.91422700881958\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:49 INFO 140171572815680] Epoch[3] Batch [30]#011Speed: 846.20 samples/sec#011loss=5.914227\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:49 INFO 140171572815680] Epoch[3] Batch[35] avg_epoch_loss=5.930669\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:49 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=35 train loss <loss>=5.938572883605957\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:49 INFO 140171572815680] Epoch[3] Batch [35]#011Speed: 1877.55 samples/sec#011loss=5.938573\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:50 INFO 140171572815680] Epoch[3] Batch[40] avg_epoch_loss=5.935803\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:50 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=40 train loss <loss>=5.972765636444092\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:50 INFO 140171572815680] Epoch[3] Batch [40]#011Speed: 839.98 samples/sec#011loss=5.972766\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:50 INFO 140171572815680] Epoch[3] Batch[45] avg_epoch_loss=5.937090\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:50 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=45 train loss <loss>=5.947647476196289\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:50 INFO 140171572815680] Epoch[3] Batch [45]#011Speed: 1794.63 samples/sec#011loss=5.947647\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:51 INFO 140171572815680] Epoch[3] Batch[50] avg_epoch_loss=5.936282\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:51 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=50 train loss <loss>=5.92884521484375\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:51 INFO 140171572815680] Epoch[3] Batch [50]#011Speed: 840.89 samples/sec#011loss=5.928845\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:51 INFO 140171572815680] Epoch[3] Batch[55] avg_epoch_loss=5.936896\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:51 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=55 train loss <loss>=5.943156623840332\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:51 INFO 140171572815680] Epoch[3] Batch [55]#011Speed: 1812.69 samples/sec#011loss=5.943157\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:52 INFO 140171572815680] Epoch[3] Batch[60] avg_epoch_loss=5.937161\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:52 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=60 train loss <loss>=5.940132331848145\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:52 INFO 140171572815680] Epoch[3] Batch [60]#011Speed: 842.49 samples/sec#011loss=5.940132\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:53 INFO 140171572815680] Epoch[3] Batch[65] avg_epoch_loss=5.937694\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:53 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=65 train loss <loss>=5.9441930770874025\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:53 INFO 140171572815680] Epoch[3] Batch [65]#011Speed: 1834.47 samples/sec#011loss=5.944193\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:53 INFO 140171572815680] Epoch[3] Batch[70] avg_epoch_loss=5.935595\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:53 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=70 train loss <loss>=5.907884120941162\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:53 INFO 140171572815680] Epoch[3] Batch [70]#011Speed: 819.74 samples/sec#011loss=5.907884\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:54 INFO 140171572815680] Epoch[3] Batch[75] avg_epoch_loss=5.936527\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:54 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=75 train loss <loss>=5.9497654914855955\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:54 INFO 140171572815680] Epoch[3] Batch [75]#011Speed: 1835.97 samples/sec#011loss=5.949765\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:54 INFO 140171572815680] Epoch[3] Batch[80] avg_epoch_loss=5.936267\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:54 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=80 train loss <loss>=5.932319831848145\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:54 INFO 140171572815680] Epoch[3] Batch [80]#011Speed: 821.54 samples/sec#011loss=5.932320\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:55 INFO 140171572815680] Epoch[3] Batch[85] avg_epoch_loss=5.938465\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:55 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=85 train loss <loss>=5.974062728881836\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:55 INFO 140171572815680] Epoch[3] Batch [85]#011Speed: 1839.05 samples/sec#011loss=5.974063\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:56 INFO 140171572815680] Epoch[3] Batch[90] avg_epoch_loss=5.936323\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:56 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=90 train loss <loss>=5.899490070343018\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:56 INFO 140171572815680] Epoch[3] Batch [90]#011Speed: 834.80 samples/sec#011loss=5.899490\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:56 INFO 140171572815680] Epoch[3] Batch[95] avg_epoch_loss=5.935975\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:56 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=95 train loss <loss>=5.929633045196534\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:56 INFO 140171572815680] Epoch[3] Batch [95]#011Speed: 1864.43 samples/sec#011loss=5.929633\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:57 INFO 140171572815680] Epoch[3] Batch[100] avg_epoch_loss=5.939207\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:57 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=100 train loss <loss>=6.001275444030762\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:57 INFO 140171572815680] Epoch[3] Batch [100]#011Speed: 833.83 samples/sec#011loss=6.001275\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:57 INFO 140171572815680] Epoch[3] Batch[105] avg_epoch_loss=5.941149\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:57 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=105 train loss <loss>=5.980373859405518\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:57 INFO 140171572815680] Epoch[3] Batch [105]#011Speed: 1832.20 samples/sec#011loss=5.980374\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:58 INFO 140171572815680] Epoch[3] Batch[110] avg_epoch_loss=5.941693\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:58 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=110 train loss <loss>=5.953228092193603\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:58 INFO 140171572815680] Epoch[3] Batch [110]#011Speed: 849.72 samples/sec#011loss=5.953228\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:58 INFO 140171572815680] Epoch[3] Batch[115] avg_epoch_loss=5.940127\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:58 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=115 train loss <loss>=5.905353260040283\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:58 INFO 140171572815680] Epoch[3] Batch [115]#011Speed: 1574.11 samples/sec#011loss=5.905353\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:59 INFO 140171572815680] Epoch[3] Batch[120] avg_epoch_loss=5.938657\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:59 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=120 train loss <loss>=5.904552555084228\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:59 INFO 140171572815680] Epoch[3] Batch [120]#011Speed: 837.68 samples/sec#011loss=5.904553\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:59 INFO 140171572815680] Epoch[3] Batch[125] avg_epoch_loss=5.939532\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:59 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=125 train loss <loss>=5.960712623596192\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:08:59 INFO 140171572815680] Epoch[3] Batch [125]#011Speed: 1847.88 samples/sec#011loss=5.960713\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:00 INFO 140171572815680] Epoch[3] Batch[130] avg_epoch_loss=5.938677\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:00 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=130 train loss <loss>=5.917131519317627\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:00 INFO 140171572815680] Epoch[3] Batch [130]#011Speed: 828.42 samples/sec#011loss=5.917132\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:00 INFO 140171572815680] Epoch[3] Batch[135] avg_epoch_loss=5.939219\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:00 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=135 train loss <loss>=5.953407669067383\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:00 INFO 140171572815680] Epoch[3] Batch [135]#011Speed: 1712.97 samples/sec#011loss=5.953408\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:01 INFO 140171572815680] Epoch[3] Batch[140] avg_epoch_loss=5.937830\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:01 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=140 train loss <loss>=5.900063133239746\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:01 INFO 140171572815680] Epoch[3] Batch [140]#011Speed: 728.89 samples/sec#011loss=5.900063\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:02 INFO 140171572815680] Epoch[3] Batch[145] avg_epoch_loss=5.939100\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:02 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=145 train loss <loss>=5.974917221069336\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:02 INFO 140171572815680] Epoch[3] Batch [145]#011Speed: 1229.27 samples/sec#011loss=5.974917\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:03 INFO 140171572815680] Epoch[3] Batch[150] avg_epoch_loss=5.939336\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:03 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=150 train loss <loss>=5.946215534210205\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:03 INFO 140171572815680] Epoch[3] Batch [150]#011Speed: 628.31 samples/sec#011loss=5.946216\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:03 INFO 140171572815680] Epoch[3] Batch[155] avg_epoch_loss=5.939583\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:03 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=155 train loss <loss>=5.9470501899719235\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:03 INFO 140171572815680] Epoch[3] Batch [155]#011Speed: 1267.12 samples/sec#011loss=5.947050\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:04 INFO 140171572815680] Epoch[3] Batch[160] avg_epoch_loss=5.939236\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:04 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=160 train loss <loss>=5.928392696380615\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:04 INFO 140171572815680] Epoch[3] Batch [160]#011Speed: 783.71 samples/sec#011loss=5.928393\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:05 INFO 140171572815680] Epoch[3] Batch[165] avg_epoch_loss=5.937793\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:05 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=165 train loss <loss>=5.8913263320922855\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:05 INFO 140171572815680] Epoch[3] Batch [165]#011Speed: 1866.59 samples/sec#011loss=5.891326\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:05 INFO 140171572815680] Epoch[3] Batch[170] avg_epoch_loss=5.938217\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:05 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=170 train loss <loss>=5.952313995361328\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:05 INFO 140171572815680] Epoch[3] Batch [170]#011Speed: 806.03 samples/sec#011loss=5.952314\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:06 INFO 140171572815680] Epoch[3] Batch[175] avg_epoch_loss=5.938208\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:06 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=175 train loss <loss>=5.937906265258789\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:06 INFO 140171572815680] Epoch[3] Batch [175]#011Speed: 1824.63 samples/sec#011loss=5.937906\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:06 INFO 140171572815680] Epoch[3] Batch[180] avg_epoch_loss=5.937360\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:06 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=180 train loss <loss>=5.907489967346192\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:06 INFO 140171572815680] Epoch[3] Batch [180]#011Speed: 819.60 samples/sec#011loss=5.907490\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:07 INFO 140171572815680] Epoch[3] Batch[185] avg_epoch_loss=5.936130\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:07 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=185 train loss <loss>=5.8915966033935545\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:07 INFO 140171572815680] Epoch[3] Batch [185]#011Speed: 1716.06 samples/sec#011loss=5.891597\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:08 INFO 140171572815680] Epoch[3] Batch[190] avg_epoch_loss=5.935820\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:08 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=190 train loss <loss>=5.92429027557373\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:08 INFO 140171572815680] Epoch[3] Batch [190]#011Speed: 832.30 samples/sec#011loss=5.924290\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:08 INFO 140171572815680] Epoch[3] Batch[195] avg_epoch_loss=5.935757\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:08 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=195 train loss <loss>=5.933377456665039\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:08 INFO 140171572815680] Epoch[3] Batch [195]#011Speed: 1663.70 samples/sec#011loss=5.933377\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:09 INFO 140171572815680] Epoch[3] Batch[200] avg_epoch_loss=5.934652\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:09 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=200 train loss <loss>=5.891315746307373\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:09 INFO 140171572815680] Epoch[3] Batch [200]#011Speed: 827.06 samples/sec#011loss=5.891316\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:09 INFO 140171572815680] Epoch[3] Batch[205] avg_epoch_loss=5.934265\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:09 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=205 train loss <loss>=5.918726539611816\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:09 INFO 140171572815680] Epoch[3] Batch [205]#011Speed: 1822.74 samples/sec#011loss=5.918727\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:10 INFO 140171572815680] Epoch[3] Batch[210] avg_epoch_loss=5.933918\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:10 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=210 train loss <loss>=5.91961784362793\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:10 INFO 140171572815680] Epoch[3] Batch [210]#011Speed: 833.13 samples/sec#011loss=5.919618\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:10 INFO 140171572815680] Epoch[3] Batch[215] avg_epoch_loss=5.933128\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:10 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=215 train loss <loss>=5.8997753143310545\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:10 INFO 140171572815680] Epoch[3] Batch [215]#011Speed: 1805.86 samples/sec#011loss=5.899775\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:11 INFO 140171572815680] Epoch[3] Batch[220] avg_epoch_loss=5.933554\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:11 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=220 train loss <loss>=5.951946449279785\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:11 INFO 140171572815680] Epoch[3] Batch [220]#011Speed: 824.49 samples/sec#011loss=5.951946\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:11 INFO 140171572815680] Epoch[3] Batch[225] avg_epoch_loss=5.933954\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:11 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=225 train loss <loss>=5.951648235321045\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:11 INFO 140171572815680] Epoch[3] Batch [225]#011Speed: 1807.33 samples/sec#011loss=5.951648\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:12 INFO 140171572815680] Epoch[3] Batch[230] avg_epoch_loss=5.932319\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:12 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, batch=230 train loss <loss>=5.8584339141845705\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:12 INFO 140171572815680] Epoch[3] Batch [230]#011Speed: 1333.99 samples/sec#011loss=5.858434\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:12 INFO 140171572815680] processed a total of 29734 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1705802925.4888883, \"EndTime\": 1705802952.489341, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 27000.0, \"count\": 1, \"min\": 27000.0, \"max\": 27000.0}}}\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:12 INFO 140171572815680] #throughput_metric: host=algo-1, train throughput=1101.2538330321945 records/second\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:12 INFO 140171572815680] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:12 INFO 140171572815680] #quality_metric: host=algo-1, epoch=3, train loss <loss>=5.93262862135924\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:13 INFO 140171572815680] Epoch[4] Batch[0] avg_epoch_loss=5.975486\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:13 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=5.9754862785339355\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:13 INFO 140171572815680] Epoch[4] Batch[5] avg_epoch_loss=5.892934\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:13 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=5.89293360710144\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:13 INFO 140171572815680] Epoch[4] Batch [5]#011Speed: 1817.64 samples/sec#011loss=5.892934\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:14 INFO 140171572815680] Epoch[4] Batch[10] avg_epoch_loss=5.877136\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:14 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=5.858178520202637\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:14 INFO 140171572815680] Epoch[4] Batch [10]#011Speed: 840.02 samples/sec#011loss=5.858179\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:14 INFO 140171572815680] Epoch[4] Batch[15] avg_epoch_loss=5.877085\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:14 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=15 train loss <loss>=5.876973438262939\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:14 INFO 140171572815680] Epoch[4] Batch [15]#011Speed: 1815.43 samples/sec#011loss=5.876973\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:15 INFO 140171572815680] Epoch[4] Batch[20] avg_epoch_loss=5.908953\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:15 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=20 train loss <loss>=6.010931777954101\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:15 INFO 140171572815680] Epoch[4] Batch [20]#011Speed: 778.30 samples/sec#011loss=6.010932\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:15 INFO 140171572815680] Epoch[4] Batch[25] avg_epoch_loss=5.913728\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:15 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=25 train loss <loss>=5.933780670166016\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:15 INFO 140171572815680] Epoch[4] Batch [25]#011Speed: 1766.08 samples/sec#011loss=5.933781\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:16 INFO 140171572815680] Epoch[4] Batch[30] avg_epoch_loss=5.905030\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:16 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=30 train loss <loss>=5.859801483154297\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:16 INFO 140171572815680] Epoch[4] Batch [30]#011Speed: 801.24 samples/sec#011loss=5.859801\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:16 INFO 140171572815680] Epoch[4] Batch[35] avg_epoch_loss=5.905635\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:16 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=35 train loss <loss>=5.909388065338135\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:16 INFO 140171572815680] Epoch[4] Batch [35]#011Speed: 1865.82 samples/sec#011loss=5.909388\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:17 INFO 140171572815680] Epoch[4] Batch[40] avg_epoch_loss=5.902619\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:17 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=40 train loss <loss>=5.880899238586426\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:17 INFO 140171572815680] Epoch[4] Batch [40]#011Speed: 842.15 samples/sec#011loss=5.880899\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:17 INFO 140171572815680] Epoch[4] Batch[45] avg_epoch_loss=5.905014\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:17 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=45 train loss <loss>=5.924654769897461\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:17 INFO 140171572815680] Epoch[4] Batch [45]#011Speed: 1906.92 samples/sec#011loss=5.924655\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:18 INFO 140171572815680] Epoch[4] Batch[50] avg_epoch_loss=5.902654\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:18 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=50 train loss <loss>=5.880941677093506\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:18 INFO 140171572815680] Epoch[4] Batch [50]#011Speed: 827.49 samples/sec#011loss=5.880942\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:19 INFO 140171572815680] Epoch[4] Batch[55] avg_epoch_loss=5.896378\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:19 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=55 train loss <loss>=5.832365608215332\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:19 INFO 140171572815680] Epoch[4] Batch [55]#011Speed: 1754.33 samples/sec#011loss=5.832366\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:19 INFO 140171572815680] Epoch[4] Batch[60] avg_epoch_loss=5.899155\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:19 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=60 train loss <loss>=5.930257034301758\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:19 INFO 140171572815680] Epoch[4] Batch [60]#011Speed: 841.15 samples/sec#011loss=5.930257\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:20 INFO 140171572815680] Epoch[4] Batch[65] avg_epoch_loss=5.901951\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:20 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=65 train loss <loss>=5.936064720153809\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:20 INFO 140171572815680] Epoch[4] Batch [65]#011Speed: 1872.21 samples/sec#011loss=5.936065\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:20 INFO 140171572815680] Epoch[4] Batch[70] avg_epoch_loss=5.902239\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:20 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=70 train loss <loss>=5.906036376953125\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:20 INFO 140171572815680] Epoch[4] Batch [70]#011Speed: 823.73 samples/sec#011loss=5.906036\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:21 INFO 140171572815680] Epoch[4] Batch[75] avg_epoch_loss=5.908017\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:21 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=75 train loss <loss>=5.990057754516601\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:21 INFO 140171572815680] Epoch[4] Batch [75]#011Speed: 1887.58 samples/sec#011loss=5.990058\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:22 INFO 140171572815680] Epoch[4] Batch[80] avg_epoch_loss=5.906898\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:22 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=80 train loss <loss>=5.889890861511231\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:22 INFO 140171572815680] Epoch[4] Batch [80]#011Speed: 784.44 samples/sec#011loss=5.889891\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:22 INFO 140171572815680] Epoch[4] Batch[85] avg_epoch_loss=5.908231\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:22 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=85 train loss <loss>=5.929830837249756\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:22 INFO 140171572815680] Epoch[4] Batch [85]#011Speed: 1790.66 samples/sec#011loss=5.929831\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:23 INFO 140171572815680] Epoch[4] Batch[90] avg_epoch_loss=5.910266\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:23 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=90 train loss <loss>=5.945269298553467\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:23 INFO 140171572815680] Epoch[4] Batch [90]#011Speed: 836.88 samples/sec#011loss=5.945269\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:23 INFO 140171572815680] Epoch[4] Batch[95] avg_epoch_loss=5.909950\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:23 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=95 train loss <loss>=5.904198932647705\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:23 INFO 140171572815680] Epoch[4] Batch [95]#011Speed: 1843.20 samples/sec#011loss=5.904199\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:24 INFO 140171572815680] Epoch[4] Batch[100] avg_epoch_loss=5.908345\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:24 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=100 train loss <loss>=5.877518844604492\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:24 INFO 140171572815680] Epoch[4] Batch [100]#011Speed: 847.55 samples/sec#011loss=5.877519\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:24 INFO 140171572815680] Epoch[4] Batch[105] avg_epoch_loss=5.908756\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:24 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=105 train loss <loss>=5.917076301574707\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:24 INFO 140171572815680] Epoch[4] Batch [105]#011Speed: 1764.40 samples/sec#011loss=5.917076\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:25 INFO 140171572815680] Epoch[4] Batch[110] avg_epoch_loss=5.910064\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:25 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=110 train loss <loss>=5.937787437438965\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:25 INFO 140171572815680] Epoch[4] Batch [110]#011Speed: 841.57 samples/sec#011loss=5.937787\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:25 INFO 140171572815680] Epoch[4] Batch[115] avg_epoch_loss=5.910135\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:25 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=115 train loss <loss>=5.911718463897705\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:25 INFO 140171572815680] Epoch[4] Batch [115]#011Speed: 1808.14 samples/sec#011loss=5.911718\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:26 INFO 140171572815680] Epoch[4] Batch[120] avg_epoch_loss=5.908737\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:26 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=120 train loss <loss>=5.876290512084961\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:26 INFO 140171572815680] Epoch[4] Batch [120]#011Speed: 843.15 samples/sec#011loss=5.876291\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:26 INFO 140171572815680] Epoch[4] Batch[125] avg_epoch_loss=5.907745\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:26 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=125 train loss <loss>=5.883743476867676\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:26 INFO 140171572815680] Epoch[4] Batch [125]#011Speed: 1844.84 samples/sec#011loss=5.883743\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:27 INFO 140171572815680] Epoch[4] Batch[130] avg_epoch_loss=5.907650\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:27 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=130 train loss <loss>=5.905263137817383\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:27 INFO 140171572815680] Epoch[4] Batch [130]#011Speed: 860.99 samples/sec#011loss=5.905263\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:27 INFO 140171572815680] Epoch[4] Batch[135] avg_epoch_loss=5.907577\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:27 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=135 train loss <loss>=5.905652713775635\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:27 INFO 140171572815680] Epoch[4] Batch [135]#011Speed: 1887.64 samples/sec#011loss=5.905653\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:28 INFO 140171572815680] Epoch[4] Batch[140] avg_epoch_loss=5.908007\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:28 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=140 train loss <loss>=5.919709396362305\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:28 INFO 140171572815680] Epoch[4] Batch [140]#011Speed: 858.01 samples/sec#011loss=5.919709\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:29 INFO 140171572815680] Epoch[4] Batch[145] avg_epoch_loss=5.908511\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:29 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=145 train loss <loss>=5.922727775573731\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:29 INFO 140171572815680] Epoch[4] Batch [145]#011Speed: 1795.35 samples/sec#011loss=5.922728\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:29 INFO 140171572815680] Epoch[4] Batch[150] avg_epoch_loss=5.908933\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:29 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=150 train loss <loss>=5.921251583099365\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:29 INFO 140171572815680] Epoch[4] Batch [150]#011Speed: 814.96 samples/sec#011loss=5.921252\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:30 INFO 140171572815680] Epoch[4] Batch[155] avg_epoch_loss=5.908485\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:30 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=155 train loss <loss>=5.894939327239991\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:30 INFO 140171572815680] Epoch[4] Batch [155]#011Speed: 1697.13 samples/sec#011loss=5.894939\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:31 INFO 140171572815680] Epoch[4] Batch[160] avg_epoch_loss=5.909312\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:31 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=160 train loss <loss>=5.935128974914551\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:31 INFO 140171572815680] Epoch[4] Batch [160]#011Speed: 811.93 samples/sec#011loss=5.935129\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:31 INFO 140171572815680] Epoch[4] Batch[165] avg_epoch_loss=5.908119\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:31 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=165 train loss <loss>=5.869711780548096\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:31 INFO 140171572815680] Epoch[4] Batch [165]#011Speed: 1857.11 samples/sec#011loss=5.869712\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:32 INFO 140171572815680] Epoch[4] Batch[170] avg_epoch_loss=5.906069\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:32 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=170 train loss <loss>=5.837981796264648\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:32 INFO 140171572815680] Epoch[4] Batch [170]#011Speed: 843.52 samples/sec#011loss=5.837982\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:32 INFO 140171572815680] Epoch[4] Batch[175] avg_epoch_loss=5.906049\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:32 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=175 train loss <loss>=5.905379390716552\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:32 INFO 140171572815680] Epoch[4] Batch [175]#011Speed: 1825.88 samples/sec#011loss=5.905379\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:33 INFO 140171572815680] Epoch[4] Batch[180] avg_epoch_loss=5.904648\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:33 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=180 train loss <loss>=5.8553242683410645\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:33 INFO 140171572815680] Epoch[4] Batch [180]#011Speed: 845.52 samples/sec#011loss=5.855324\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:33 INFO 140171572815680] Epoch[4] Batch[185] avg_epoch_loss=5.903831\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:33 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=185 train loss <loss>=5.874267673492431\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:33 INFO 140171572815680] Epoch[4] Batch [185]#011Speed: 1759.59 samples/sec#011loss=5.874268\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:34 INFO 140171572815680] Epoch[4] Batch[190] avg_epoch_loss=5.903288\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:34 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=190 train loss <loss>=5.88309965133667\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:34 INFO 140171572815680] Epoch[4] Batch [190]#011Speed: 839.83 samples/sec#011loss=5.883100\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:34 INFO 140171572815680] Epoch[4] Batch[195] avg_epoch_loss=5.903536\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:34 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=195 train loss <loss>=5.913005065917969\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:34 INFO 140171572815680] Epoch[4] Batch [195]#011Speed: 1759.46 samples/sec#011loss=5.913005\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:35 INFO 140171572815680] Epoch[4] Batch[200] avg_epoch_loss=5.902460\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:35 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=200 train loss <loss>=5.8602550506591795\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:35 INFO 140171572815680] Epoch[4] Batch [200]#011Speed: 818.58 samples/sec#011loss=5.860255\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:35 INFO 140171572815680] Epoch[4] Batch[205] avg_epoch_loss=5.902018\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:35 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=205 train loss <loss>=5.884253978729248\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:35 INFO 140171572815680] Epoch[4] Batch [205]#011Speed: 1866.00 samples/sec#011loss=5.884254\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:36 INFO 140171572815680] Epoch[4] Batch[210] avg_epoch_loss=5.900825\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:36 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=210 train loss <loss>=5.85169038772583\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:36 INFO 140171572815680] Epoch[4] Batch [210]#011Speed: 825.64 samples/sec#011loss=5.851690\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:36 INFO 140171572815680] Epoch[4] Batch[215] avg_epoch_loss=5.900733\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:36 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=215 train loss <loss>=5.896859264373779\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:36 INFO 140171572815680] Epoch[4] Batch [215]#011Speed: 1848.10 samples/sec#011loss=5.896859\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:37 INFO 140171572815680] Epoch[4] Batch[220] avg_epoch_loss=5.899979\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:37 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=220 train loss <loss>=5.867396259307862\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:37 INFO 140171572815680] Epoch[4] Batch [220]#011Speed: 801.79 samples/sec#011loss=5.867396\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:38 INFO 140171572815680] Epoch[4] Batch[225] avg_epoch_loss=5.898505\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:38 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=225 train loss <loss>=5.8333664894104\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:38 INFO 140171572815680] Epoch[4] Batch [225]#011Speed: 1889.03 samples/sec#011loss=5.833366\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:38 INFO 140171572815680] Epoch[4] Batch[230] avg_epoch_loss=5.898244\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:38 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, batch=230 train loss <loss>=5.886416435241699\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:38 INFO 140171572815680] Epoch[4] Batch [230]#011Speed: 1277.36 samples/sec#011loss=5.886416\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:38 INFO 140171572815680] processed a total of 29806 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1705802952.4894292, \"EndTime\": 1705802978.752021, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 26261.690855026245, \"count\": 1, \"min\": 26261.690855026245, \"max\": 26261.690855026245}}}\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:38 INFO 140171572815680] #throughput_metric: host=algo-1, train throughput=1134.9547950081515 records/second\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:38 INFO 140171572815680] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:38 INFO 140171572815680] #quality_metric: host=algo-1, epoch=4, train loss <loss>=5.898252182252417\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:39 INFO 140171572815680] Epoch[5] Batch[0] avg_epoch_loss=5.939454\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:39 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=5.939454078674316\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:39 INFO 140171572815680] Epoch[5] Batch[5] avg_epoch_loss=5.894683\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:39 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=5.8946826457977295\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:39 INFO 140171572815680] Epoch[5] Batch [5]#011Speed: 1827.48 samples/sec#011loss=5.894683\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:40 INFO 140171572815680] Epoch[5] Batch[10] avg_epoch_loss=5.896947\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:40 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=5.899664688110351\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:40 INFO 140171572815680] Epoch[5] Batch [10]#011Speed: 831.02 samples/sec#011loss=5.899665\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:40 INFO 140171572815680] Epoch[5] Batch[15] avg_epoch_loss=5.893179\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:40 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=15 train loss <loss>=5.8848876953125\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:40 INFO 140171572815680] Epoch[5] Batch [15]#011Speed: 1694.05 samples/sec#011loss=5.884888\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:41 INFO 140171572815680] Epoch[5] Batch[20] avg_epoch_loss=5.894135\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:41 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=20 train loss <loss>=5.897194957733154\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:41 INFO 140171572815680] Epoch[5] Batch [20]#011Speed: 843.30 samples/sec#011loss=5.897195\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:41 INFO 140171572815680] Epoch[5] Batch[25] avg_epoch_loss=5.901855\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:41 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=25 train loss <loss>=5.934277725219727\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:41 INFO 140171572815680] Epoch[5] Batch [25]#011Speed: 1796.21 samples/sec#011loss=5.934278\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:42 INFO 140171572815680] Epoch[5] Batch[30] avg_epoch_loss=5.900768\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:42 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=30 train loss <loss>=5.8951188087463375\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:42 INFO 140171572815680] Epoch[5] Batch [30]#011Speed: 836.46 samples/sec#011loss=5.895119\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:42 INFO 140171572815680] Epoch[5] Batch[35] avg_epoch_loss=5.899483\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:42 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=35 train loss <loss>=5.89151782989502\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:42 INFO 140171572815680] Epoch[5] Batch [35]#011Speed: 1851.73 samples/sec#011loss=5.891518\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:43 INFO 140171572815680] Epoch[5] Batch[40] avg_epoch_loss=5.900689\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:43 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=40 train loss <loss>=5.909371185302734\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:43 INFO 140171572815680] Epoch[5] Batch [40]#011Speed: 831.06 samples/sec#011loss=5.909371\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:44 INFO 140171572815680] Epoch[5] Batch[45] avg_epoch_loss=5.904656\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:44 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=45 train loss <loss>=5.937178993225098\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:44 INFO 140171572815680] Epoch[5] Batch [45]#011Speed: 1844.32 samples/sec#011loss=5.937179\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:44 INFO 140171572815680] Epoch[5] Batch[50] avg_epoch_loss=5.907653\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:44 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=50 train loss <loss>=5.9352294921875\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:44 INFO 140171572815680] Epoch[5] Batch [50]#011Speed: 858.01 samples/sec#011loss=5.935229\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:45 INFO 140171572815680] Epoch[5] Batch[55] avg_epoch_loss=5.906473\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:45 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=55 train loss <loss>=5.894435405731201\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:45 INFO 140171572815680] Epoch[5] Batch [55]#011Speed: 1878.22 samples/sec#011loss=5.894435\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:45 INFO 140171572815680] Epoch[5] Batch[60] avg_epoch_loss=5.902186\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:45 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=60 train loss <loss>=5.854167175292969\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:45 INFO 140171572815680] Epoch[5] Batch [60]#011Speed: 852.55 samples/sec#011loss=5.854167\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:46 INFO 140171572815680] Epoch[5] Batch[65] avg_epoch_loss=5.899987\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:46 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=65 train loss <loss>=5.873167896270752\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:46 INFO 140171572815680] Epoch[5] Batch [65]#011Speed: 1873.09 samples/sec#011loss=5.873168\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:47 INFO 140171572815680] Epoch[5] Batch[70] avg_epoch_loss=5.897048\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:47 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=70 train loss <loss>=5.8582456588745115\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:47 INFO 140171572815680] Epoch[5] Batch [70]#011Speed: 817.28 samples/sec#011loss=5.858246\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:47 INFO 140171572815680] Epoch[5] Batch[75] avg_epoch_loss=5.894785\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:47 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=75 train loss <loss>=5.862657070159912\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:47 INFO 140171572815680] Epoch[5] Batch [75]#011Speed: 1853.32 samples/sec#011loss=5.862657\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:48 INFO 140171572815680] Epoch[5] Batch[80] avg_epoch_loss=5.896070\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:48 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=80 train loss <loss>=5.915593814849854\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:48 INFO 140171572815680] Epoch[5] Batch [80]#011Speed: 840.47 samples/sec#011loss=5.915594\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:48 INFO 140171572815680] Epoch[5] Batch[85] avg_epoch_loss=5.913846\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:48 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=85 train loss <loss>=6.201828289031982\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:48 INFO 140171572815680] Epoch[5] Batch [85]#011Speed: 1863.81 samples/sec#011loss=6.201828\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:49 INFO 140171572815680] Epoch[5] Batch[90] avg_epoch_loss=5.913271\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:49 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=90 train loss <loss>=5.903375339508057\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:49 INFO 140171572815680] Epoch[5] Batch [90]#011Speed: 845.94 samples/sec#011loss=5.903375\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:49 INFO 140171572815680] Epoch[5] Batch[95] avg_epoch_loss=5.910776\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:49 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=95 train loss <loss>=5.865364074707031\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:49 INFO 140171572815680] Epoch[5] Batch [95]#011Speed: 1825.99 samples/sec#011loss=5.865364\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:50 INFO 140171572815680] Epoch[5] Batch[100] avg_epoch_loss=5.915681\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:50 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=100 train loss <loss>=6.00986967086792\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:50 INFO 140171572815680] Epoch[5] Batch [100]#011Speed: 834.50 samples/sec#011loss=6.009870\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:50 INFO 140171572815680] Epoch[5] Batch[105] avg_epoch_loss=5.913161\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:50 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=105 train loss <loss>=5.862237930297852\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:50 INFO 140171572815680] Epoch[5] Batch [105]#011Speed: 1810.67 samples/sec#011loss=5.862238\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:51 INFO 140171572815680] Epoch[5] Batch[110] avg_epoch_loss=5.912109\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:51 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=110 train loss <loss>=5.889818572998047\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:51 INFO 140171572815680] Epoch[5] Batch [110]#011Speed: 821.26 samples/sec#011loss=5.889819\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:51 INFO 140171572815680] Epoch[5] Batch[115] avg_epoch_loss=5.912312\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:51 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=115 train loss <loss>=5.916806411743164\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:51 INFO 140171572815680] Epoch[5] Batch [115]#011Speed: 1833.54 samples/sec#011loss=5.916806\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:52 INFO 140171572815680] Epoch[5] Batch[120] avg_epoch_loss=5.911737\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:52 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=120 train loss <loss>=5.8983964920043945\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:52 INFO 140171572815680] Epoch[5] Batch [120]#011Speed: 851.32 samples/sec#011loss=5.898396\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:52 INFO 140171572815680] Epoch[5] Batch[125] avg_epoch_loss=5.909714\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:52 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=125 train loss <loss>=5.860769844055175\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:52 INFO 140171572815680] Epoch[5] Batch [125]#011Speed: 1864.93 samples/sec#011loss=5.860770\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:53 INFO 140171572815680] Epoch[5] Batch[130] avg_epoch_loss=5.908135\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:53 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=130 train loss <loss>=5.868339920043946\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:53 INFO 140171572815680] Epoch[5] Batch [130]#011Speed: 849.48 samples/sec#011loss=5.868340\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:54 INFO 140171572815680] Epoch[5] Batch[135] avg_epoch_loss=5.905956\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:54 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=135 train loss <loss>=5.8488719940185545\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:54 INFO 140171572815680] Epoch[5] Batch [135]#011Speed: 1872.80 samples/sec#011loss=5.848872\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:54 INFO 140171572815680] Epoch[5] Batch[140] avg_epoch_loss=5.904302\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:54 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=140 train loss <loss>=5.859310531616211\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:54 INFO 140171572815680] Epoch[5] Batch [140]#011Speed: 813.63 samples/sec#011loss=5.859311\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:55 INFO 140171572815680] Epoch[5] Batch[145] avg_epoch_loss=5.901842\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:55 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=145 train loss <loss>=5.832455635070801\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:55 INFO 140171572815680] Epoch[5] Batch [145]#011Speed: 1888.05 samples/sec#011loss=5.832456\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:55 INFO 140171572815680] Epoch[5] Batch[150] avg_epoch_loss=5.899795\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:55 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=150 train loss <loss>=5.840043067932129\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:55 INFO 140171572815680] Epoch[5] Batch [150]#011Speed: 817.82 samples/sec#011loss=5.840043\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:56 INFO 140171572815680] Epoch[5] Batch[155] avg_epoch_loss=5.899049\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:56 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=155 train loss <loss>=5.876504802703858\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:56 INFO 140171572815680] Epoch[5] Batch [155]#011Speed: 1885.55 samples/sec#011loss=5.876505\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:57 INFO 140171572815680] Epoch[5] Batch[160] avg_epoch_loss=5.898211\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:57 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=160 train loss <loss>=5.872077560424804\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:57 INFO 140171572815680] Epoch[5] Batch [160]#011Speed: 836.53 samples/sec#011loss=5.872078\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:57 INFO 140171572815680] Epoch[5] Batch[165] avg_epoch_loss=5.897721\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:57 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=165 train loss <loss>=5.881925964355469\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:57 INFO 140171572815680] Epoch[5] Batch [165]#011Speed: 1882.75 samples/sec#011loss=5.881926\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:58 INFO 140171572815680] Epoch[5] Batch[170] avg_epoch_loss=5.895992\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:58 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=170 train loss <loss>=5.838594245910644\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:58 INFO 140171572815680] Epoch[5] Batch [170]#011Speed: 844.85 samples/sec#011loss=5.838594\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:58 INFO 140171572815680] Epoch[5] Batch[175] avg_epoch_loss=5.895969\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:58 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=175 train loss <loss>=5.895189476013184\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:58 INFO 140171572815680] Epoch[5] Batch [175]#011Speed: 1908.02 samples/sec#011loss=5.895189\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:59 INFO 140171572815680] Epoch[5] Batch[180] avg_epoch_loss=5.894889\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:59 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=180 train loss <loss>=5.856884002685547\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:59 INFO 140171572815680] Epoch[5] Batch [180]#011Speed: 837.43 samples/sec#011loss=5.856884\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:59 INFO 140171572815680] Epoch[5] Batch[185] avg_epoch_loss=5.894133\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:59 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=185 train loss <loss>=5.866768646240234\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:09:59 INFO 140171572815680] Epoch[5] Batch [185]#011Speed: 1853.64 samples/sec#011loss=5.866769\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:00 INFO 140171572815680] Epoch[5] Batch[190] avg_epoch_loss=5.892586\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:00 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=190 train loss <loss>=5.83501558303833\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:00 INFO 140171572815680] Epoch[5] Batch [190]#011Speed: 811.56 samples/sec#011loss=5.835016\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:00 INFO 140171572815680] Epoch[5] Batch[195] avg_epoch_loss=5.892267\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:00 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=195 train loss <loss>=5.880094337463379\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:00 INFO 140171572815680] Epoch[5] Batch [195]#011Speed: 1757.15 samples/sec#011loss=5.880094\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:01 INFO 140171572815680] Epoch[5] Batch[200] avg_epoch_loss=5.891785\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:01 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=200 train loss <loss>=5.87287015914917\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:01 INFO 140171572815680] Epoch[5] Batch [200]#011Speed: 799.91 samples/sec#011loss=5.872870\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:01 INFO 140171572815680] Epoch[5] Batch[205] avg_epoch_loss=5.891312\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:01 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=205 train loss <loss>=5.87231855392456\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:01 INFO 140171572815680] Epoch[5] Batch [205]#011Speed: 1651.57 samples/sec#011loss=5.872319\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:02 INFO 140171572815680] Epoch[5] Batch[210] avg_epoch_loss=5.890449\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:02 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=210 train loss <loss>=5.854879379272461\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:02 INFO 140171572815680] Epoch[5] Batch [210]#011Speed: 628.83 samples/sec#011loss=5.854879\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:03 INFO 140171572815680] Epoch[5] Batch[215] avg_epoch_loss=5.890538\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:03 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=215 train loss <loss>=5.8942865371704105\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:03 INFO 140171572815680] Epoch[5] Batch [215]#011Speed: 1337.11 samples/sec#011loss=5.894287\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:04 INFO 140171572815680] Epoch[5] Batch[220] avg_epoch_loss=5.890562\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:04 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=220 train loss <loss>=5.891601181030273\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:04 INFO 140171572815680] Epoch[5] Batch [220]#011Speed: 632.92 samples/sec#011loss=5.891601\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:04 INFO 140171572815680] Epoch[5] Batch[225] avg_epoch_loss=5.889194\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:04 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=225 train loss <loss>=5.828732681274414\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:04 INFO 140171572815680] Epoch[5] Batch [225]#011Speed: 1791.84 samples/sec#011loss=5.828733\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:05 INFO 140171572815680] Epoch[5] Batch[230] avg_epoch_loss=5.888800\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:05 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, batch=230 train loss <loss>=5.87098970413208\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:05 INFO 140171572815680] Epoch[5] Batch [230]#011Speed: 1224.20 samples/sec#011loss=5.870990\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:05 INFO 140171572815680] processed a total of 29975 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1705802978.7521217, \"EndTime\": 1705803005.59743, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 26844.836235046387, \"count\": 1, \"min\": 26844.836235046387, \"max\": 26844.836235046387}}}\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:05 INFO 140171572815680] #throughput_metric: host=algo-1, train throughput=1116.59539686631 records/second\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:05 INFO 140171572815680] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:05 INFO 140171572815680] #quality_metric: host=algo-1, epoch=5, train loss <loss>=5.89734871438209\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:06 INFO 140171572815680] Epoch[6] Batch[0] avg_epoch_loss=5.915110\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:06 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=5.915110111236572\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:06 INFO 140171572815680] Epoch[6] Batch[5] avg_epoch_loss=5.891341\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:06 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=5.891341129938762\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:06 INFO 140171572815680] Epoch[6] Batch [5]#011Speed: 1721.39 samples/sec#011loss=5.891341\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:07 INFO 140171572815680] Epoch[6] Batch[10] avg_epoch_loss=5.903894\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:07 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=5.918958473205566\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:07 INFO 140171572815680] Epoch[6] Batch [10]#011Speed: 827.68 samples/sec#011loss=5.918958\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:07 INFO 140171572815680] Epoch[6] Batch[15] avg_epoch_loss=5.894678\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:07 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=15 train loss <loss>=5.874402618408203\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:07 INFO 140171572815680] Epoch[6] Batch [15]#011Speed: 1825.51 samples/sec#011loss=5.874403\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:08 INFO 140171572815680] Epoch[6] Batch[20] avg_epoch_loss=5.879356\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:08 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=20 train loss <loss>=5.830322742462158\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:08 INFO 140171572815680] Epoch[6] Batch [20]#011Speed: 820.42 samples/sec#011loss=5.830323\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:08 INFO 140171572815680] Epoch[6] Batch[25] avg_epoch_loss=5.882096\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:08 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=25 train loss <loss>=5.893607425689697\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:08 INFO 140171572815680] Epoch[6] Batch [25]#011Speed: 1824.08 samples/sec#011loss=5.893607\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:09 INFO 140171572815680] Epoch[6] Batch[30] avg_epoch_loss=5.883868\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:09 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=30 train loss <loss>=5.8930840492248535\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:09 INFO 140171572815680] Epoch[6] Batch [30]#011Speed: 840.34 samples/sec#011loss=5.893084\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:09 INFO 140171572815680] Epoch[6] Batch[35] avg_epoch_loss=5.887260\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:09 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=35 train loss <loss>=5.908288383483887\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:09 INFO 140171572815680] Epoch[6] Batch [35]#011Speed: 1749.06 samples/sec#011loss=5.908288\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:10 INFO 140171572815680] Epoch[6] Batch[40] avg_epoch_loss=5.885706\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:10 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=40 train loss <loss>=5.874515247344971\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:10 INFO 140171572815680] Epoch[6] Batch [40]#011Speed: 846.74 samples/sec#011loss=5.874515\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:10 INFO 140171572815680] Epoch[6] Batch[45] avg_epoch_loss=5.886665\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:10 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=45 train loss <loss>=5.894533443450928\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:10 INFO 140171572815680] Epoch[6] Batch [45]#011Speed: 1802.89 samples/sec#011loss=5.894533\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:11 INFO 140171572815680] Epoch[6] Batch[50] avg_epoch_loss=5.884400\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:11 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=50 train loss <loss>=5.863557529449463\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:11 INFO 140171572815680] Epoch[6] Batch [50]#011Speed: 832.47 samples/sec#011loss=5.863558\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:12 INFO 140171572815680] Epoch[6] Batch[55] avg_epoch_loss=5.884990\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:12 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=55 train loss <loss>=5.891011428833008\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:12 INFO 140171572815680] Epoch[6] Batch [55]#011Speed: 1771.91 samples/sec#011loss=5.891011\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:12 INFO 140171572815680] Epoch[6] Batch[60] avg_epoch_loss=5.884869\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:12 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=60 train loss <loss>=5.883517169952393\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:12 INFO 140171572815680] Epoch[6] Batch [60]#011Speed: 835.59 samples/sec#011loss=5.883517\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:13 INFO 140171572815680] Epoch[6] Batch[65] avg_epoch_loss=5.881818\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:13 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=65 train loss <loss>=5.844584178924561\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:13 INFO 140171572815680] Epoch[6] Batch [65]#011Speed: 1838.06 samples/sec#011loss=5.844584\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:13 INFO 140171572815680] Epoch[6] Batch[70] avg_epoch_loss=5.882021\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:13 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=70 train loss <loss>=5.884703636169434\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:13 INFO 140171572815680] Epoch[6] Batch [70]#011Speed: 820.32 samples/sec#011loss=5.884704\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:14 INFO 140171572815680] Epoch[6] Batch[75] avg_epoch_loss=5.880600\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:14 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=75 train loss <loss>=5.860431289672851\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:14 INFO 140171572815680] Epoch[6] Batch [75]#011Speed: 1690.43 samples/sec#011loss=5.860431\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:15 INFO 140171572815680] Epoch[6] Batch[80] avg_epoch_loss=5.879095\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:15 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=80 train loss <loss>=5.856211471557617\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:15 INFO 140171572815680] Epoch[6] Batch [80]#011Speed: 834.39 samples/sec#011loss=5.856211\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:15 INFO 140171572815680] Epoch[6] Batch[85] avg_epoch_loss=5.877759\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:15 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=85 train loss <loss>=5.856120300292969\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:15 INFO 140171572815680] Epoch[6] Batch [85]#011Speed: 1780.10 samples/sec#011loss=5.856120\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:16 INFO 140171572815680] Epoch[6] Batch[90] avg_epoch_loss=5.875416\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:16 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=90 train loss <loss>=5.835110282897949\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:16 INFO 140171572815680] Epoch[6] Batch [90]#011Speed: 836.67 samples/sec#011loss=5.835110\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:16 INFO 140171572815680] Epoch[6] Batch[95] avg_epoch_loss=5.875170\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:16 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=95 train loss <loss>=5.870690727233887\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:16 INFO 140171572815680] Epoch[6] Batch [95]#011Speed: 1859.12 samples/sec#011loss=5.870691\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:17 INFO 140171572815680] Epoch[6] Batch[100] avg_epoch_loss=5.872830\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:17 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=100 train loss <loss>=5.827900409698486\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:17 INFO 140171572815680] Epoch[6] Batch [100]#011Speed: 808.30 samples/sec#011loss=5.827900\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:17 INFO 140171572815680] Epoch[6] Batch[105] avg_epoch_loss=5.871939\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:17 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=105 train loss <loss>=5.853940010070801\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:17 INFO 140171572815680] Epoch[6] Batch [105]#011Speed: 1838.32 samples/sec#011loss=5.853940\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:18 INFO 140171572815680] Epoch[6] Batch[110] avg_epoch_loss=5.869236\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:18 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=110 train loss <loss>=5.8119291305542\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:18 INFO 140171572815680] Epoch[6] Batch [110]#011Speed: 822.78 samples/sec#011loss=5.811929\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:18 INFO 140171572815680] Epoch[6] Batch[115] avg_epoch_loss=5.867391\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:18 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=115 train loss <loss>=5.826450443267822\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:18 INFO 140171572815680] Epoch[6] Batch [115]#011Speed: 1877.59 samples/sec#011loss=5.826450\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:19 INFO 140171572815680] Epoch[6] Batch[120] avg_epoch_loss=5.865980\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:19 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=120 train loss <loss>=5.833239555358887\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:19 INFO 140171572815680] Epoch[6] Batch [120]#011Speed: 823.30 samples/sec#011loss=5.833240\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:20 INFO 140171572815680] Epoch[6] Batch[125] avg_epoch_loss=5.863689\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:20 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=125 train loss <loss>=5.80823450088501\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:20 INFO 140171572815680] Epoch[6] Batch [125]#011Speed: 1649.57 samples/sec#011loss=5.808235\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:20 INFO 140171572815680] Epoch[6] Batch[130] avg_epoch_loss=5.863500\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:20 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=130 train loss <loss>=5.858734226226806\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:20 INFO 140171572815680] Epoch[6] Batch [130]#011Speed: 828.08 samples/sec#011loss=5.858734\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:21 INFO 140171572815680] Epoch[6] Batch[135] avg_epoch_loss=5.865685\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:21 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=135 train loss <loss>=5.92295732498169\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:21 INFO 140171572815680] Epoch[6] Batch [135]#011Speed: 1794.82 samples/sec#011loss=5.922957\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:21 INFO 140171572815680] Epoch[6] Batch[140] avg_epoch_loss=5.864086\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:21 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=140 train loss <loss>=5.820589542388916\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:21 INFO 140171572815680] Epoch[6] Batch [140]#011Speed: 839.84 samples/sec#011loss=5.820590\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:22 INFO 140171572815680] Epoch[6] Batch[145] avg_epoch_loss=5.864534\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:22 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=145 train loss <loss>=5.8771570205688475\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:22 INFO 140171572815680] Epoch[6] Batch [145]#011Speed: 1817.52 samples/sec#011loss=5.877157\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:23 INFO 140171572815680] Epoch[6] Batch[150] avg_epoch_loss=5.863940\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:23 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=150 train loss <loss>=5.846592140197754\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:23 INFO 140171572815680] Epoch[6] Batch [150]#011Speed: 849.34 samples/sec#011loss=5.846592\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:23 INFO 140171572815680] Epoch[6] Batch[155] avg_epoch_loss=5.864578\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:23 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=155 train loss <loss>=5.883841609954834\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:23 INFO 140171572815680] Epoch[6] Batch [155]#011Speed: 1819.41 samples/sec#011loss=5.883842\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:24 INFO 140171572815680] Epoch[6] Batch[160] avg_epoch_loss=5.863212\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:24 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=160 train loss <loss>=5.820600318908691\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:24 INFO 140171572815680] Epoch[6] Batch [160]#011Speed: 835.04 samples/sec#011loss=5.820600\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:24 INFO 140171572815680] Epoch[6] Batch[165] avg_epoch_loss=5.862790\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:24 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=165 train loss <loss>=5.849186038970947\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:24 INFO 140171572815680] Epoch[6] Batch [165]#011Speed: 1851.31 samples/sec#011loss=5.849186\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:25 INFO 140171572815680] Epoch[6] Batch[170] avg_epoch_loss=5.862883\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:25 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=170 train loss <loss>=5.865999507904053\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:25 INFO 140171572815680] Epoch[6] Batch [170]#011Speed: 775.33 samples/sec#011loss=5.866000\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:25 INFO 140171572815680] Epoch[6] Batch[175] avg_epoch_loss=5.862779\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:25 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=175 train loss <loss>=5.859204578399658\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:25 INFO 140171572815680] Epoch[6] Batch [175]#011Speed: 1569.35 samples/sec#011loss=5.859205\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:26 INFO 140171572815680] Epoch[6] Batch[180] avg_epoch_loss=5.861574\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:26 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=180 train loss <loss>=5.819173526763916\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:26 INFO 140171572815680] Epoch[6] Batch [180]#011Speed: 822.45 samples/sec#011loss=5.819174\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:26 INFO 140171572815680] Epoch[6] Batch[185] avg_epoch_loss=5.860583\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:26 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=185 train loss <loss>=5.824707126617431\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:26 INFO 140171572815680] Epoch[6] Batch [185]#011Speed: 1836.23 samples/sec#011loss=5.824707\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:27 INFO 140171572815680] Epoch[6] Batch[190] avg_epoch_loss=5.861144\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:27 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=190 train loss <loss>=5.881999397277832\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:27 INFO 140171572815680] Epoch[6] Batch [190]#011Speed: 754.15 samples/sec#011loss=5.881999\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:28 INFO 140171572815680] Epoch[6] Batch[195] avg_epoch_loss=5.861613\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:28 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=195 train loss <loss>=5.87954740524292\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:28 INFO 140171572815680] Epoch[6] Batch [195]#011Speed: 1905.07 samples/sec#011loss=5.879547\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:28 INFO 140171572815680] Epoch[6] Batch[200] avg_epoch_loss=5.862898\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:28 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=200 train loss <loss>=5.9132530212402346\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:28 INFO 140171572815680] Epoch[6] Batch [200]#011Speed: 813.39 samples/sec#011loss=5.913253\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:29 INFO 140171572815680] Epoch[6] Batch[205] avg_epoch_loss=5.862192\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:29 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=205 train loss <loss>=5.8338042259216305\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:29 INFO 140171572815680] Epoch[6] Batch [205]#011Speed: 1840.04 samples/sec#011loss=5.833804\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:29 INFO 140171572815680] Epoch[6] Batch[210] avg_epoch_loss=5.861750\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:29 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=210 train loss <loss>=5.843559646606446\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:29 INFO 140171572815680] Epoch[6] Batch [210]#011Speed: 825.80 samples/sec#011loss=5.843560\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:30 INFO 140171572815680] Epoch[6] Batch[215] avg_epoch_loss=5.861741\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:30 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=215 train loss <loss>=5.861339664459228\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:30 INFO 140171572815680] Epoch[6] Batch [215]#011Speed: 1824.35 samples/sec#011loss=5.861340\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:31 INFO 140171572815680] Epoch[6] Batch[220] avg_epoch_loss=5.862640\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:31 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=220 train loss <loss>=5.901499271392822\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:31 INFO 140171572815680] Epoch[6] Batch [220]#011Speed: 811.30 samples/sec#011loss=5.901499\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:31 INFO 140171572815680] Epoch[6] Batch[225] avg_epoch_loss=5.862493\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:31 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=225 train loss <loss>=5.855989551544189\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:31 INFO 140171572815680] Epoch[6] Batch [225]#011Speed: 1802.05 samples/sec#011loss=5.855990\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:31 INFO 140171572815680] Epoch[6] Batch[230] avg_epoch_loss=5.862448\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:31 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, batch=230 train loss <loss>=5.8603941917419435\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:31 INFO 140171572815680] Epoch[6] Batch [230]#011Speed: 1372.01 samples/sec#011loss=5.860394\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:32 INFO 140171572815680] processed a total of 29814 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1705803005.597542, \"EndTime\": 1705803032.0609052, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 26462.606191635132, \"count\": 1, \"min\": 26462.606191635132, \"max\": 26462.606191635132}}}\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:32 INFO 140171572815680] #throughput_metric: host=algo-1, train throughput=1126.6399644899645 records/second\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:32 INFO 140171572815680] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:32 INFO 140171572815680] #quality_metric: host=algo-1, epoch=6, train loss <loss>=5.862045541853353\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:32 INFO 140171572815680] Epoch[7] Batch[0] avg_epoch_loss=5.767648\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:32 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=5.767647743225098\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:32 INFO 140171572815680] Epoch[7] Batch[5] avg_epoch_loss=5.818789\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:32 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=5.81878940264384\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:32 INFO 140171572815680] Epoch[7] Batch [5]#011Speed: 1781.51 samples/sec#011loss=5.818789\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:33 INFO 140171572815680] Epoch[7] Batch[10] avg_epoch_loss=5.822449\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:33 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=5.826840877532959\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:33 INFO 140171572815680] Epoch[7] Batch [10]#011Speed: 845.66 samples/sec#011loss=5.826841\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:34 INFO 140171572815680] Epoch[7] Batch[15] avg_epoch_loss=5.834891\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:34 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=15 train loss <loss>=5.862263584136963\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:34 INFO 140171572815680] Epoch[7] Batch [15]#011Speed: 1864.21 samples/sec#011loss=5.862264\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:34 INFO 140171572815680] Epoch[7] Batch[20] avg_epoch_loss=5.834503\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:34 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=20 train loss <loss>=5.8332624435424805\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:34 INFO 140171572815680] Epoch[7] Batch [20]#011Speed: 848.27 samples/sec#011loss=5.833262\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:35 INFO 140171572815680] Epoch[7] Batch[25] avg_epoch_loss=5.845743\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:35 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=25 train loss <loss>=5.892949104309082\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:35 INFO 140171572815680] Epoch[7] Batch [25]#011Speed: 1861.25 samples/sec#011loss=5.892949\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:35 INFO 140171572815680] Epoch[7] Batch[30] avg_epoch_loss=5.856420\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:35 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=30 train loss <loss>=5.911938667297363\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:35 INFO 140171572815680] Epoch[7] Batch [30]#011Speed: 820.49 samples/sec#011loss=5.911939\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:36 INFO 140171572815680] Epoch[7] Batch[35] avg_epoch_loss=5.858496\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:36 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=35 train loss <loss>=5.87136812210083\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:36 INFO 140171572815680] Epoch[7] Batch [35]#011Speed: 1741.88 samples/sec#011loss=5.871368\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:37 INFO 140171572815680] Epoch[7] Batch[40] avg_epoch_loss=5.857071\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:37 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=40 train loss <loss>=5.846815586090088\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:37 INFO 140171572815680] Epoch[7] Batch [40]#011Speed: 794.36 samples/sec#011loss=5.846816\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:37 INFO 140171572815680] Epoch[7] Batch[45] avg_epoch_loss=5.857886\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:37 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=45 train loss <loss>=5.864561939239502\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:37 INFO 140171572815680] Epoch[7] Batch [45]#011Speed: 1690.73 samples/sec#011loss=5.864562\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:38 INFO 140171572815680] Epoch[7] Batch[50] avg_epoch_loss=5.857664\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:38 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=50 train loss <loss>=5.855628871917725\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:38 INFO 140171572815680] Epoch[7] Batch [50]#011Speed: 848.60 samples/sec#011loss=5.855629\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:38 INFO 140171572815680] Epoch[7] Batch[55] avg_epoch_loss=5.857248\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:38 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=55 train loss <loss>=5.853004360198975\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:38 INFO 140171572815680] Epoch[7] Batch [55]#011Speed: 1807.47 samples/sec#011loss=5.853004\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:39 INFO 140171572815680] Epoch[7] Batch[60] avg_epoch_loss=5.854774\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:39 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=60 train loss <loss>=5.82706356048584\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:39 INFO 140171572815680] Epoch[7] Batch [60]#011Speed: 858.48 samples/sec#011loss=5.827064\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:39 INFO 140171572815680] Epoch[7] Batch[65] avg_epoch_loss=5.852926\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:39 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=65 train loss <loss>=5.830385398864746\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:39 INFO 140171572815680] Epoch[7] Batch [65]#011Speed: 1840.34 samples/sec#011loss=5.830385\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:40 INFO 140171572815680] Epoch[7] Batch[70] avg_epoch_loss=5.854767\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:40 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=70 train loss <loss>=5.879065322875976\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:40 INFO 140171572815680] Epoch[7] Batch [70]#011Speed: 826.96 samples/sec#011loss=5.879065\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:40 INFO 140171572815680] Epoch[7] Batch[75] avg_epoch_loss=5.857526\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:40 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=75 train loss <loss>=5.8966965675354\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:40 INFO 140171572815680] Epoch[7] Batch [75]#011Speed: 1818.80 samples/sec#011loss=5.896697\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:41 INFO 140171572815680] Epoch[7] Batch[80] avg_epoch_loss=5.858891\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:41 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=80 train loss <loss>=5.879645538330078\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:41 INFO 140171572815680] Epoch[7] Batch [80]#011Speed: 821.94 samples/sec#011loss=5.879646\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:41 INFO 140171572815680] Epoch[7] Batch[85] avg_epoch_loss=5.857681\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:41 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=85 train loss <loss>=5.838069820404053\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:41 INFO 140171572815680] Epoch[7] Batch [85]#011Speed: 1864.88 samples/sec#011loss=5.838070\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:42 INFO 140171572815680] Epoch[7] Batch[90] avg_epoch_loss=5.860009\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:42 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=90 train loss <loss>=5.900049781799316\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:42 INFO 140171572815680] Epoch[7] Batch [90]#011Speed: 866.02 samples/sec#011loss=5.900050\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:42 INFO 140171572815680] Epoch[7] Batch[95] avg_epoch_loss=5.861267\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:42 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=95 train loss <loss>=5.8841602325439455\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:42 INFO 140171572815680] Epoch[7] Batch [95]#011Speed: 1883.88 samples/sec#011loss=5.884160\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:43 INFO 140171572815680] Epoch[7] Batch[100] avg_epoch_loss=5.862123\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:43 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=100 train loss <loss>=5.8785624504089355\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:43 INFO 140171572815680] Epoch[7] Batch [100]#011Speed: 848.40 samples/sec#011loss=5.878562\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:44 INFO 140171572815680] Epoch[7] Batch[105] avg_epoch_loss=5.860098\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:44 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=105 train loss <loss>=5.819207572937012\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:44 INFO 140171572815680] Epoch[7] Batch [105]#011Speed: 1888.18 samples/sec#011loss=5.819208\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:44 INFO 140171572815680] Epoch[7] Batch[110] avg_epoch_loss=5.862355\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:44 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=110 train loss <loss>=5.910183525085449\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:44 INFO 140171572815680] Epoch[7] Batch [110]#011Speed: 820.46 samples/sec#011loss=5.910184\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:45 INFO 140171572815680] Epoch[7] Batch[115] avg_epoch_loss=5.865102\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:45 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=115 train loss <loss>=5.926096630096436\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:45 INFO 140171572815680] Epoch[7] Batch [115]#011Speed: 1891.97 samples/sec#011loss=5.926097\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:45 INFO 140171572815680] Epoch[7] Batch[120] avg_epoch_loss=5.864539\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:45 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=120 train loss <loss>=5.851482963562011\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:45 INFO 140171572815680] Epoch[7] Batch [120]#011Speed: 839.42 samples/sec#011loss=5.851483\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:46 INFO 140171572815680] Epoch[7] Batch[125] avg_epoch_loss=5.863869\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:46 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=125 train loss <loss>=5.847641468048096\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:46 INFO 140171572815680] Epoch[7] Batch [125]#011Speed: 1842.16 samples/sec#011loss=5.847641\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:47 INFO 140171572815680] Epoch[7] Batch[130] avg_epoch_loss=5.863722\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:47 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=130 train loss <loss>=5.860022735595703\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:47 INFO 140171572815680] Epoch[7] Batch [130]#011Speed: 857.73 samples/sec#011loss=5.860023\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:47 INFO 140171572815680] Epoch[7] Batch[135] avg_epoch_loss=5.864289\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:47 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=135 train loss <loss>=5.879156398773193\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:47 INFO 140171572815680] Epoch[7] Batch [135]#011Speed: 1846.68 samples/sec#011loss=5.879156\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:48 INFO 140171572815680] Epoch[7] Batch[140] avg_epoch_loss=5.865227\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:48 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=140 train loss <loss>=5.890737724304199\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:48 INFO 140171572815680] Epoch[7] Batch [140]#011Speed: 834.31 samples/sec#011loss=5.890738\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:48 INFO 140171572815680] Epoch[7] Batch[145] avg_epoch_loss=5.862760\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:48 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=145 train loss <loss>=5.793194389343261\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:48 INFO 140171572815680] Epoch[7] Batch [145]#011Speed: 1864.41 samples/sec#011loss=5.793194\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:49 INFO 140171572815680] Epoch[7] Batch[150] avg_epoch_loss=5.862255\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:49 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=150 train loss <loss>=5.847491359710693\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:49 INFO 140171572815680] Epoch[7] Batch [150]#011Speed: 829.88 samples/sec#011loss=5.847491\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:49 INFO 140171572815680] Epoch[7] Batch[155] avg_epoch_loss=5.871225\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:49 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=155 train loss <loss>=6.142116832733154\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:49 INFO 140171572815680] Epoch[7] Batch [155]#011Speed: 1856.51 samples/sec#011loss=6.142117\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:50 INFO 140171572815680] Epoch[7] Batch[160] avg_epoch_loss=5.871280\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:50 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=160 train loss <loss>=5.87300853729248\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:50 INFO 140171572815680] Epoch[7] Batch [160]#011Speed: 855.90 samples/sec#011loss=5.873009\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:50 INFO 140171572815680] Epoch[7] Batch[165] avg_epoch_loss=5.871830\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:50 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=165 train loss <loss>=5.8895245552062985\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:50 INFO 140171572815680] Epoch[7] Batch [165]#011Speed: 1615.38 samples/sec#011loss=5.889525\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:51 INFO 140171572815680] Epoch[7] Batch[170] avg_epoch_loss=5.870174\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:51 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=170 train loss <loss>=5.815219879150391\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:51 INFO 140171572815680] Epoch[7] Batch [170]#011Speed: 839.16 samples/sec#011loss=5.815220\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:51 INFO 140171572815680] Epoch[7] Batch[175] avg_epoch_loss=5.868703\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:51 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=175 train loss <loss>=5.818366432189942\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:51 INFO 140171572815680] Epoch[7] Batch [175]#011Speed: 1766.33 samples/sec#011loss=5.818366\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:52 INFO 140171572815680] Epoch[7] Batch[180] avg_epoch_loss=5.867204\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:52 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=180 train loss <loss>=5.814451408386231\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:52 INFO 140171572815680] Epoch[7] Batch [180]#011Speed: 823.61 samples/sec#011loss=5.814451\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:53 INFO 140171572815680] Epoch[7] Batch[185] avg_epoch_loss=5.867230\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:53 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=185 train loss <loss>=5.8681800842285154\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:53 INFO 140171572815680] Epoch[7] Batch [185]#011Speed: 1864.51 samples/sec#011loss=5.868180\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:53 INFO 140171572815680] Epoch[7] Batch[190] avg_epoch_loss=5.866729\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:53 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=190 train loss <loss>=5.848090362548828\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:53 INFO 140171572815680] Epoch[7] Batch [190]#011Speed: 846.78 samples/sec#011loss=5.848090\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:54 INFO 140171572815680] Epoch[7] Batch[195] avg_epoch_loss=5.866539\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:54 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=195 train loss <loss>=5.859267711639404\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:54 INFO 140171572815680] Epoch[7] Batch [195]#011Speed: 1864.21 samples/sec#011loss=5.859268\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:54 INFO 140171572815680] Epoch[7] Batch[200] avg_epoch_loss=5.865583\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:54 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=200 train loss <loss>=5.828135776519775\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:54 INFO 140171572815680] Epoch[7] Batch [200]#011Speed: 810.12 samples/sec#011loss=5.828136\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:55 INFO 140171572815680] Epoch[7] Batch[205] avg_epoch_loss=5.865309\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:55 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=205 train loss <loss>=5.854267883300781\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:55 INFO 140171572815680] Epoch[7] Batch [205]#011Speed: 1728.40 samples/sec#011loss=5.854268\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:56 INFO 140171572815680] Epoch[7] Batch[210] avg_epoch_loss=5.865294\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:56 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=210 train loss <loss>=5.864688491821289\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:56 INFO 140171572815680] Epoch[7] Batch [210]#011Speed: 849.77 samples/sec#011loss=5.864688\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:56 INFO 140171572815680] Epoch[7] Batch[215] avg_epoch_loss=5.865351\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:56 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=215 train loss <loss>=5.8677520751953125\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:56 INFO 140171572815680] Epoch[7] Batch [215]#011Speed: 1858.12 samples/sec#011loss=5.867752\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:57 INFO 140171572815680] Epoch[7] Batch[220] avg_epoch_loss=5.865724\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:57 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=220 train loss <loss>=5.8818258285522464\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:57 INFO 140171572815680] Epoch[7] Batch [220]#011Speed: 819.71 samples/sec#011loss=5.881826\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:57 INFO 140171572815680] Epoch[7] Batch[225] avg_epoch_loss=5.865950\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:57 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=225 train loss <loss>=5.875933265686035\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:57 INFO 140171572815680] Epoch[7] Batch [225]#011Speed: 1861.34 samples/sec#011loss=5.875933\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:58 INFO 140171572815680] Epoch[7] Batch[230] avg_epoch_loss=5.865121\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:58 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, batch=230 train loss <loss>=5.827685928344726\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:58 INFO 140171572815680] Epoch[7] Batch [230]#011Speed: 1236.83 samples/sec#011loss=5.827686\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:58 INFO 140171572815680] processed a total of 29975 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1705803032.0610063, \"EndTime\": 1705803058.3103065, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 26248.860359191895, \"count\": 1, \"min\": 26248.860359191895, \"max\": 26248.860359191895}}}\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:58 INFO 140171572815680] #throughput_metric: host=algo-1, train throughput=1141.947431761735 records/second\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:58 INFO 140171572815680] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:58 INFO 140171572815680] #quality_metric: host=algo-1, epoch=7, train loss <loss>=5.872450619555535\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:58 INFO 140171572815680] Epoch[8] Batch[0] avg_epoch_loss=5.877300\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:58 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=5.877299785614014\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:59 INFO 140171572815680] Epoch[8] Batch[5] avg_epoch_loss=5.958802\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:59 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=5.958801507949829\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:59 INFO 140171572815680] Epoch[8] Batch [5]#011Speed: 1826.45 samples/sec#011loss=5.958802\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:59 INFO 140171572815680] Epoch[8] Batch[10] avg_epoch_loss=5.992755\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:59 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=6.033498096466064\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:10:59 INFO 140171572815680] Epoch[8] Batch [10]#011Speed: 817.40 samples/sec#011loss=6.033498\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:00 INFO 140171572815680] Epoch[8] Batch[15] avg_epoch_loss=5.986139\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:00 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=15 train loss <loss>=5.971584415435791\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:00 INFO 140171572815680] Epoch[8] Batch [15]#011Speed: 1845.50 samples/sec#011loss=5.971584\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:01 INFO 140171572815680] Epoch[8] Batch[20] avg_epoch_loss=6.015137\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:01 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=20 train loss <loss>=6.107930755615234\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:01 INFO 140171572815680] Epoch[8] Batch [20]#011Speed: 837.75 samples/sec#011loss=6.107931\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:01 INFO 140171572815680] Epoch[8] Batch[25] avg_epoch_loss=5.997826\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:01 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=25 train loss <loss>=5.925120067596436\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:01 INFO 140171572815680] Epoch[8] Batch [25]#011Speed: 1558.23 samples/sec#011loss=5.925120\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:02 INFO 140171572815680] Epoch[8] Batch[30] avg_epoch_loss=5.982048\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:02 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=30 train loss <loss>=5.900004959106445\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:02 INFO 140171572815680] Epoch[8] Batch [30]#011Speed: 662.82 samples/sec#011loss=5.900005\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:02 INFO 140171572815680] Epoch[8] Batch[35] avg_epoch_loss=5.964409\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:02 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=35 train loss <loss>=5.8550468444824215\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:02 INFO 140171572815680] Epoch[8] Batch [35]#011Speed: 1307.60 samples/sec#011loss=5.855047\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:03 INFO 140171572815680] Epoch[8] Batch[40] avg_epoch_loss=5.954211\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:03 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=40 train loss <loss>=5.8807862281799315\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:03 INFO 140171572815680] Epoch[8] Batch [40]#011Speed: 614.52 samples/sec#011loss=5.880786\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:04 INFO 140171572815680] Epoch[8] Batch[45] avg_epoch_loss=5.948022\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:04 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=45 train loss <loss>=5.897271347045899\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:04 INFO 140171572815680] Epoch[8] Batch [45]#011Speed: 1890.70 samples/sec#011loss=5.897271\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:05 INFO 140171572815680] Epoch[8] Batch[50] avg_epoch_loss=5.943173\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:05 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=50 train loss <loss>=5.898560619354248\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:05 INFO 140171572815680] Epoch[8] Batch [50]#011Speed: 799.88 samples/sec#011loss=5.898561\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:05 INFO 140171572815680] Epoch[8] Batch[55] avg_epoch_loss=5.938725\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:05 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=55 train loss <loss>=5.893352127075195\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:05 INFO 140171572815680] Epoch[8] Batch [55]#011Speed: 1880.94 samples/sec#011loss=5.893352\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:06 INFO 140171572815680] Epoch[8] Batch[60] avg_epoch_loss=5.932375\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:06 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=60 train loss <loss>=5.861258792877197\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:06 INFO 140171572815680] Epoch[8] Batch [60]#011Speed: 837.45 samples/sec#011loss=5.861259\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:06 INFO 140171572815680] Epoch[8] Batch[65] avg_epoch_loss=5.929249\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:06 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=65 train loss <loss>=5.891115474700928\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:06 INFO 140171572815680] Epoch[8] Batch [65]#011Speed: 1839.57 samples/sec#011loss=5.891115\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:07 INFO 140171572815680] Epoch[8] Batch[70] avg_epoch_loss=5.927317\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:07 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=70 train loss <loss>=5.901805400848389\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:07 INFO 140171572815680] Epoch[8] Batch [70]#011Speed: 833.04 samples/sec#011loss=5.901805\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:07 INFO 140171572815680] Epoch[8] Batch[75] avg_epoch_loss=5.926298\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:07 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=75 train loss <loss>=5.911830234527588\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:07 INFO 140171572815680] Epoch[8] Batch [75]#011Speed: 1887.38 samples/sec#011loss=5.911830\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:08 INFO 140171572815680] Epoch[8] Batch[80] avg_epoch_loss=5.926159\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:08 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=80 train loss <loss>=5.924040794372559\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:08 INFO 140171572815680] Epoch[8] Batch [80]#011Speed: 812.10 samples/sec#011loss=5.924041\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:08 INFO 140171572815680] Epoch[8] Batch[85] avg_epoch_loss=5.922585\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:08 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=85 train loss <loss>=5.86469841003418\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:08 INFO 140171572815680] Epoch[8] Batch [85]#011Speed: 1826.14 samples/sec#011loss=5.864698\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:09 INFO 140171572815680] Epoch[8] Batch[90] avg_epoch_loss=5.919548\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:09 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=90 train loss <loss>=5.867298603057861\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:09 INFO 140171572815680] Epoch[8] Batch [90]#011Speed: 812.71 samples/sec#011loss=5.867299\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:09 INFO 140171572815680] Epoch[8] Batch[95] avg_epoch_loss=5.916828\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:09 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=95 train loss <loss>=5.86733570098877\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:09 INFO 140171572815680] Epoch[8] Batch [95]#011Speed: 1799.81 samples/sec#011loss=5.867336\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:10 INFO 140171572815680] Epoch[8] Batch[100] avg_epoch_loss=5.914582\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:10 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=100 train loss <loss>=5.8714546203613285\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:10 INFO 140171572815680] Epoch[8] Batch [100]#011Speed: 826.12 samples/sec#011loss=5.871455\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:11 INFO 140171572815680] Epoch[8] Batch[105] avg_epoch_loss=5.911363\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:11 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=105 train loss <loss>=5.846334838867188\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:11 INFO 140171572815680] Epoch[8] Batch [105]#011Speed: 1687.97 samples/sec#011loss=5.846335\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:11 INFO 140171572815680] Epoch[8] Batch[110] avg_epoch_loss=5.909173\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:11 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=110 train loss <loss>=5.862751293182373\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:11 INFO 140171572815680] Epoch[8] Batch [110]#011Speed: 826.95 samples/sec#011loss=5.862751\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:12 INFO 140171572815680] Epoch[8] Batch[115] avg_epoch_loss=5.907217\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:12 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=115 train loss <loss>=5.863794136047363\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:12 INFO 140171572815680] Epoch[8] Batch [115]#011Speed: 1681.85 samples/sec#011loss=5.863794\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:13 INFO 140171572815680] Epoch[8] Batch[120] avg_epoch_loss=5.906005\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:13 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=120 train loss <loss>=5.877895641326904\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:13 INFO 140171572815680] Epoch[8] Batch [120]#011Speed: 826.83 samples/sec#011loss=5.877896\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:13 INFO 140171572815680] Epoch[8] Batch[125] avg_epoch_loss=5.905788\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:13 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=125 train loss <loss>=5.900514888763428\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:13 INFO 140171572815680] Epoch[8] Batch [125]#011Speed: 1850.63 samples/sec#011loss=5.900515\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:14 INFO 140171572815680] Epoch[8] Batch[130] avg_epoch_loss=5.905051\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:14 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=130 train loss <loss>=5.88649730682373\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:14 INFO 140171572815680] Epoch[8] Batch [130]#011Speed: 789.39 samples/sec#011loss=5.886497\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:14 INFO 140171572815680] Epoch[8] Batch[135] avg_epoch_loss=5.903959\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:14 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=135 train loss <loss>=5.875338459014893\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:14 INFO 140171572815680] Epoch[8] Batch [135]#011Speed: 1686.92 samples/sec#011loss=5.875338\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:15 INFO 140171572815680] Epoch[8] Batch[140] avg_epoch_loss=5.903246\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:15 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=140 train loss <loss>=5.883841323852539\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:15 INFO 140171572815680] Epoch[8] Batch [140]#011Speed: 820.11 samples/sec#011loss=5.883841\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:15 INFO 140171572815680] Epoch[8] Batch[145] avg_epoch_loss=5.902349\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:15 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=145 train loss <loss>=5.877055931091308\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:15 INFO 140171572815680] Epoch[8] Batch [145]#011Speed: 1834.08 samples/sec#011loss=5.877056\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:16 INFO 140171572815680] Epoch[8] Batch[150] avg_epoch_loss=5.900694\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:16 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=150 train loss <loss>=5.852388954162597\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:16 INFO 140171572815680] Epoch[8] Batch [150]#011Speed: 841.68 samples/sec#011loss=5.852389\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:16 INFO 140171572815680] Epoch[8] Batch[155] avg_epoch_loss=5.900374\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:16 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=155 train loss <loss>=5.890711688995362\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:16 INFO 140171572815680] Epoch[8] Batch [155]#011Speed: 1632.25 samples/sec#011loss=5.890712\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:17 INFO 140171572815680] Epoch[8] Batch[160] avg_epoch_loss=5.898575\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:17 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=160 train loss <loss>=5.842434883117676\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:17 INFO 140171572815680] Epoch[8] Batch [160]#011Speed: 822.61 samples/sec#011loss=5.842435\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:17 INFO 140171572815680] Epoch[8] Batch[165] avg_epoch_loss=5.896720\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:17 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=165 train loss <loss>=5.83700532913208\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:17 INFO 140171572815680] Epoch[8] Batch [165]#011Speed: 1871.45 samples/sec#011loss=5.837005\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:18 INFO 140171572815680] Epoch[8] Batch[170] avg_epoch_loss=5.896018\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:18 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=170 train loss <loss>=5.872692012786866\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:18 INFO 140171572815680] Epoch[8] Batch [170]#011Speed: 832.08 samples/sec#011loss=5.872692\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:19 INFO 140171572815680] Epoch[8] Batch[175] avg_epoch_loss=5.893701\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:19 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=175 train loss <loss>=5.8144783020019535\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:19 INFO 140171572815680] Epoch[8] Batch [175]#011Speed: 1842.18 samples/sec#011loss=5.814478\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:19 INFO 140171572815680] Epoch[8] Batch[180] avg_epoch_loss=5.893139\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:19 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=180 train loss <loss>=5.873323822021485\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:19 INFO 140171572815680] Epoch[8] Batch [180]#011Speed: 843.34 samples/sec#011loss=5.873324\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:20 INFO 140171572815680] Epoch[8] Batch[185] avg_epoch_loss=5.891643\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:20 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=185 train loss <loss>=5.837505435943603\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:20 INFO 140171572815680] Epoch[8] Batch [185]#011Speed: 1739.47 samples/sec#011loss=5.837505\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:20 INFO 140171572815680] Epoch[8] Batch[190] avg_epoch_loss=5.889158\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:20 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=190 train loss <loss>=5.796700286865234\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:20 INFO 140171572815680] Epoch[8] Batch [190]#011Speed: 801.93 samples/sec#011loss=5.796700\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:21 INFO 140171572815680] Epoch[8] Batch[195] avg_epoch_loss=5.888758\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:21 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=195 train loss <loss>=5.8734842300415036\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:21 INFO 140171572815680] Epoch[8] Batch [195]#011Speed: 1702.84 samples/sec#011loss=5.873484\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:22 INFO 140171572815680] Epoch[8] Batch[200] avg_epoch_loss=5.888768\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:22 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=200 train loss <loss>=5.889151954650879\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:22 INFO 140171572815680] Epoch[8] Batch [200]#011Speed: 833.33 samples/sec#011loss=5.889152\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:22 INFO 140171572815680] Epoch[8] Batch[205] avg_epoch_loss=5.889261\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:22 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=205 train loss <loss>=5.90911636352539\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:22 INFO 140171572815680] Epoch[8] Batch [205]#011Speed: 1872.37 samples/sec#011loss=5.909116\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:23 INFO 140171572815680] Epoch[8] Batch[210] avg_epoch_loss=5.888526\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:23 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=210 train loss <loss>=5.858203983306884\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:23 INFO 140171572815680] Epoch[8] Batch [210]#011Speed: 803.31 samples/sec#011loss=5.858204\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:23 INFO 140171572815680] Epoch[8] Batch[215] avg_epoch_loss=5.887501\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:23 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=215 train loss <loss>=5.844285774230957\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:23 INFO 140171572815680] Epoch[8] Batch [215]#011Speed: 1855.11 samples/sec#011loss=5.844286\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:24 INFO 140171572815680] Epoch[8] Batch[220] avg_epoch_loss=5.887087\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:24 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=220 train loss <loss>=5.869179916381836\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:24 INFO 140171572815680] Epoch[8] Batch [220]#011Speed: 803.33 samples/sec#011loss=5.869180\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:24 INFO 140171572815680] Epoch[8] Batch[225] avg_epoch_loss=5.885311\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:24 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=225 train loss <loss>=5.80683126449585\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:24 INFO 140171572815680] Epoch[8] Batch [225]#011Speed: 1850.45 samples/sec#011loss=5.806831\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:25 INFO 140171572815680] Epoch[8] Batch[230] avg_epoch_loss=5.885007\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:25 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, batch=230 train loss <loss>=5.87123966217041\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:25 INFO 140171572815680] Epoch[8] Batch [230]#011Speed: 1405.30 samples/sec#011loss=5.871240\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:25 INFO 140171572815680] processed a total of 29692 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1705803058.3104153, \"EndTime\": 1705803085.28865, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 26977.800369262695, \"count\": 1, \"min\": 26977.800369262695, \"max\": 26977.800369262695}}}\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:25 INFO 140171572815680] #throughput_metric: host=algo-1, train throughput=1100.6022820154196 records/second\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:25 INFO 140171572815680] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:25 INFO 140171572815680] #quality_metric: host=algo-1, epoch=8, train loss <loss>=5.884498281725522\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:25 INFO 140171572815680] Epoch[9] Batch[0] avg_epoch_loss=5.881311\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:25 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=5.881310939788818\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:26 INFO 140171572815680] Epoch[9] Batch[5] avg_epoch_loss=5.839958\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:26 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=5.8399577140808105\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:26 INFO 140171572815680] Epoch[9] Batch [5]#011Speed: 1760.04 samples/sec#011loss=5.839958\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:26 INFO 140171572815680] Epoch[9] Batch[10] avg_epoch_loss=5.876678\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:26 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=5.920742988586426\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:26 INFO 140171572815680] Epoch[9] Batch [10]#011Speed: 836.37 samples/sec#011loss=5.920743\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:27 INFO 140171572815680] Epoch[9] Batch[15] avg_epoch_loss=5.865909\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:27 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=15 train loss <loss>=5.842217540740966\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:27 INFO 140171572815680] Epoch[9] Batch [15]#011Speed: 1799.69 samples/sec#011loss=5.842218\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:28 INFO 140171572815680] Epoch[9] Batch[20] avg_epoch_loss=5.864128\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:28 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=20 train loss <loss>=5.858427333831787\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:28 INFO 140171572815680] Epoch[9] Batch [20]#011Speed: 840.89 samples/sec#011loss=5.858427\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:28 INFO 140171572815680] Epoch[9] Batch[25] avg_epoch_loss=5.865565\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:28 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=25 train loss <loss>=5.871600341796875\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:28 INFO 140171572815680] Epoch[9] Batch [25]#011Speed: 1727.01 samples/sec#011loss=5.871600\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:29 INFO 140171572815680] Epoch[9] Batch[30] avg_epoch_loss=5.873444\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:29 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=30 train loss <loss>=5.914413166046143\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:29 INFO 140171572815680] Epoch[9] Batch [30]#011Speed: 838.60 samples/sec#011loss=5.914413\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:29 INFO 140171572815680] Epoch[9] Batch[35] avg_epoch_loss=5.875058\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:29 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=35 train loss <loss>=5.88506383895874\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:29 INFO 140171572815680] Epoch[9] Batch [35]#011Speed: 1789.70 samples/sec#011loss=5.885064\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:30 INFO 140171572815680] Epoch[9] Batch[40] avg_epoch_loss=5.874678\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:30 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=40 train loss <loss>=5.8719440460205075\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:30 INFO 140171572815680] Epoch[9] Batch [40]#011Speed: 850.75 samples/sec#011loss=5.871944\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:30 INFO 140171572815680] Epoch[9] Batch[45] avg_epoch_loss=5.874797\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:30 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=45 train loss <loss>=5.875769424438476\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:30 INFO 140171572815680] Epoch[9] Batch [45]#011Speed: 1851.29 samples/sec#011loss=5.875769\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:31 INFO 140171572815680] Epoch[9] Batch[50] avg_epoch_loss=5.873981\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:31 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=50 train loss <loss>=5.8664830207824705\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:31 INFO 140171572815680] Epoch[9] Batch [50]#011Speed: 775.83 samples/sec#011loss=5.866483\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:31 INFO 140171572815680] Epoch[9] Batch[55] avg_epoch_loss=5.874799\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:31 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=55 train loss <loss>=5.883137512207031\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:31 INFO 140171572815680] Epoch[9] Batch [55]#011Speed: 1820.15 samples/sec#011loss=5.883138\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:32 INFO 140171572815680] Epoch[9] Batch[60] avg_epoch_loss=5.869628\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:32 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=60 train loss <loss>=5.811707210540772\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:32 INFO 140171572815680] Epoch[9] Batch [60]#011Speed: 821.11 samples/sec#011loss=5.811707\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:32 INFO 140171572815680] Epoch[9] Batch[65] avg_epoch_loss=5.871557\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:32 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=65 train loss <loss>=5.895102691650391\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:32 INFO 140171572815680] Epoch[9] Batch [65]#011Speed: 1886.00 samples/sec#011loss=5.895103\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:33 INFO 140171572815680] Epoch[9] Batch[70] avg_epoch_loss=5.867838\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:33 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=70 train loss <loss>=5.818738079071045\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:33 INFO 140171572815680] Epoch[9] Batch [70]#011Speed: 793.37 samples/sec#011loss=5.818738\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:34 INFO 140171572815680] Epoch[9] Batch[75] avg_epoch_loss=5.867054\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:34 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=75 train loss <loss>=5.855927467346191\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:34 INFO 140171572815680] Epoch[9] Batch [75]#011Speed: 1890.86 samples/sec#011loss=5.855927\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:34 INFO 140171572815680] Epoch[9] Batch[80] avg_epoch_loss=5.866522\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:34 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=80 train loss <loss>=5.858432388305664\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:34 INFO 140171572815680] Epoch[9] Batch [80]#011Speed: 840.39 samples/sec#011loss=5.858432\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:35 INFO 140171572815680] Epoch[9] Batch[85] avg_epoch_loss=5.864429\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:35 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=85 train loss <loss>=5.830523204803467\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:35 INFO 140171572815680] Epoch[9] Batch [85]#011Speed: 1851.27 samples/sec#011loss=5.830523\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:35 INFO 140171572815680] Epoch[9] Batch[90] avg_epoch_loss=5.865716\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:35 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=90 train loss <loss>=5.8878580093383786\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:35 INFO 140171572815680] Epoch[9] Batch [90]#011Speed: 824.23 samples/sec#011loss=5.887858\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:36 INFO 140171572815680] Epoch[9] Batch[95] avg_epoch_loss=5.867429\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:36 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=95 train loss <loss>=5.89860782623291\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:36 INFO 140171572815680] Epoch[9] Batch [95]#011Speed: 1632.62 samples/sec#011loss=5.898608\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:37 INFO 140171572815680] Epoch[9] Batch[100] avg_epoch_loss=5.868739\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:37 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=100 train loss <loss>=5.893886280059815\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:37 INFO 140171572815680] Epoch[9] Batch [100]#011Speed: 816.67 samples/sec#011loss=5.893886\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:37 INFO 140171572815680] Epoch[9] Batch[105] avg_epoch_loss=5.868366\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:37 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=105 train loss <loss>=5.860829162597656\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:37 INFO 140171572815680] Epoch[9] Batch [105]#011Speed: 1824.39 samples/sec#011loss=5.860829\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:38 INFO 140171572815680] Epoch[9] Batch[110] avg_epoch_loss=5.865466\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:38 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=110 train loss <loss>=5.803994655609131\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:38 INFO 140171572815680] Epoch[9] Batch [110]#011Speed: 832.01 samples/sec#011loss=5.803995\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:38 INFO 140171572815680] Epoch[9] Batch[115] avg_epoch_loss=5.863684\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:38 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=115 train loss <loss>=5.824108600616455\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:38 INFO 140171572815680] Epoch[9] Batch [115]#011Speed: 1747.74 samples/sec#011loss=5.824109\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:39 INFO 140171572815680] Epoch[9] Batch[120] avg_epoch_loss=5.864237\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:39 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=120 train loss <loss>=5.877076530456543\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:39 INFO 140171572815680] Epoch[9] Batch [120]#011Speed: 821.55 samples/sec#011loss=5.877077\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:39 INFO 140171572815680] Epoch[9] Batch[125] avg_epoch_loss=5.863221\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:39 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=125 train loss <loss>=5.838635349273682\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:39 INFO 140171572815680] Epoch[9] Batch [125]#011Speed: 1630.58 samples/sec#011loss=5.838635\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:40 INFO 140171572815680] Epoch[9] Batch[130] avg_epoch_loss=5.863315\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:40 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=130 train loss <loss>=5.865688037872315\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:40 INFO 140171572815680] Epoch[9] Batch [130]#011Speed: 834.23 samples/sec#011loss=5.865688\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:40 INFO 140171572815680] Epoch[9] Batch[135] avg_epoch_loss=5.861543\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:40 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=135 train loss <loss>=5.815117835998535\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:40 INFO 140171572815680] Epoch[9] Batch [135]#011Speed: 1629.01 samples/sec#011loss=5.815118\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:41 INFO 140171572815680] Epoch[9] Batch[140] avg_epoch_loss=5.859634\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:41 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=140 train loss <loss>=5.807698345184326\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:41 INFO 140171572815680] Epoch[9] Batch [140]#011Speed: 801.00 samples/sec#011loss=5.807698\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:42 INFO 140171572815680] Epoch[9] Batch[145] avg_epoch_loss=5.858964\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:42 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=145 train loss <loss>=5.840062141418457\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:42 INFO 140171572815680] Epoch[9] Batch [145]#011Speed: 1843.03 samples/sec#011loss=5.840062\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:42 INFO 140171572815680] Epoch[9] Batch[150] avg_epoch_loss=5.859112\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:42 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=150 train loss <loss>=5.863436698913574\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:42 INFO 140171572815680] Epoch[9] Batch [150]#011Speed: 743.12 samples/sec#011loss=5.863437\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:43 INFO 140171572815680] Epoch[9] Batch[155] avg_epoch_loss=5.860199\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:43 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=155 train loss <loss>=5.893019962310791\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:43 INFO 140171572815680] Epoch[9] Batch [155]#011Speed: 1823.26 samples/sec#011loss=5.893020\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:44 INFO 140171572815680] Epoch[9] Batch[160] avg_epoch_loss=5.860430\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:44 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=160 train loss <loss>=5.867656707763672\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:44 INFO 140171572815680] Epoch[9] Batch [160]#011Speed: 831.83 samples/sec#011loss=5.867657\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:44 INFO 140171572815680] Epoch[9] Batch[165] avg_epoch_loss=5.860076\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:44 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=165 train loss <loss>=5.848660087585449\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:44 INFO 140171572815680] Epoch[9] Batch [165]#011Speed: 1808.88 samples/sec#011loss=5.848660\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:45 INFO 140171572815680] Epoch[9] Batch[170] avg_epoch_loss=5.860329\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:45 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=170 train loss <loss>=5.8687457084655765\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:45 INFO 140171572815680] Epoch[9] Batch [170]#011Speed: 839.52 samples/sec#011loss=5.868746\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:45 INFO 140171572815680] Epoch[9] Batch[175] avg_epoch_loss=5.860441\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:45 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=175 train loss <loss>=5.8642762184143065\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:45 INFO 140171572815680] Epoch[9] Batch [175]#011Speed: 1632.60 samples/sec#011loss=5.864276\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:46 INFO 140171572815680] Epoch[9] Batch[180] avg_epoch_loss=5.859626\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:46 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=180 train loss <loss>=5.830929470062256\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:46 INFO 140171572815680] Epoch[9] Batch [180]#011Speed: 820.72 samples/sec#011loss=5.830929\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:46 INFO 140171572815680] Epoch[9] Batch[185] avg_epoch_loss=5.859435\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:46 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=185 train loss <loss>=5.8525138854980465\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:46 INFO 140171572815680] Epoch[9] Batch [185]#011Speed: 1776.98 samples/sec#011loss=5.852514\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:47 INFO 140171572815680] Epoch[9] Batch[190] avg_epoch_loss=5.858853\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:47 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=190 train loss <loss>=5.837209320068359\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:47 INFO 140171572815680] Epoch[9] Batch [190]#011Speed: 815.35 samples/sec#011loss=5.837209\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:47 INFO 140171572815680] Epoch[9] Batch[195] avg_epoch_loss=5.858972\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:47 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=195 train loss <loss>=5.863508987426758\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:47 INFO 140171572815680] Epoch[9] Batch [195]#011Speed: 1845.08 samples/sec#011loss=5.863509\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:48 INFO 140171572815680] Epoch[9] Batch[200] avg_epoch_loss=5.858955\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:48 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=200 train loss <loss>=5.858300495147705\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:48 INFO 140171572815680] Epoch[9] Batch [200]#011Speed: 840.84 samples/sec#011loss=5.858300\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:49 INFO 140171572815680] Epoch[9] Batch[205] avg_epoch_loss=5.859036\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:49 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=205 train loss <loss>=5.862285804748535\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:49 INFO 140171572815680] Epoch[9] Batch [205]#011Speed: 1617.87 samples/sec#011loss=5.862286\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:49 INFO 140171572815680] Epoch[9] Batch[210] avg_epoch_loss=5.858302\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:49 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=210 train loss <loss>=5.828062343597412\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:49 INFO 140171572815680] Epoch[9] Batch [210]#011Speed: 855.08 samples/sec#011loss=5.828062\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:50 INFO 140171572815680] Epoch[9] Batch[215] avg_epoch_loss=5.858567\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:50 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=215 train loss <loss>=5.869756603240967\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:50 INFO 140171572815680] Epoch[9] Batch [215]#011Speed: 1868.07 samples/sec#011loss=5.869757\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:50 INFO 140171572815680] Epoch[9] Batch[220] avg_epoch_loss=5.858549\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:50 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=220 train loss <loss>=5.85775785446167\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:50 INFO 140171572815680] Epoch[9] Batch [220]#011Speed: 832.70 samples/sec#011loss=5.857758\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:51 INFO 140171572815680] Epoch[9] Batch[225] avg_epoch_loss=5.857781\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:51 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=225 train loss <loss>=5.823846340179443\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:51 INFO 140171572815680] Epoch[9] Batch [225]#011Speed: 1670.05 samples/sec#011loss=5.823846\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:51 INFO 140171572815680] Epoch[9] Batch[230] avg_epoch_loss=5.857460\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:51 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, batch=230 train loss <loss>=5.842943572998047\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:51 INFO 140171572815680] Epoch[9] Batch [230]#011Speed: 1166.18 samples/sec#011loss=5.842944\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:52 INFO 140171572815680] processed a total of 30021 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1705803085.2887511, \"EndTime\": 1705803112.1098688, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 26820.60742378235, \"count\": 1, \"min\": 26820.60742378235, \"max\": 26820.60742378235}}}\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:52 INFO 140171572815680] #throughput_metric: host=algo-1, train throughput=1119.3194818616078 records/second\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:52 INFO 140171572815680] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:52 INFO 140171572815680] #quality_metric: host=algo-1, epoch=9, train loss <loss>=5.856606237939063\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:52 INFO 140171572815680] Final loss: 5.856606237939063 (occurred at epoch 9)\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:52 INFO 140171572815680] #quality_metric: host=algo-1, train final_loss <loss>=5.856606237939063\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:52 INFO 140171572815680] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:52 WARNING 140171572815680] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:52 INFO 140171572815680] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1705803112.1099696, \"EndTime\": 1705803112.171584, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 60.495853424072266, \"count\": 1, \"min\": 60.495853424072266, \"max\": 60.495853424072266}}}\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:52 INFO 140171572815680] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1705803112.1716683, \"EndTime\": 1705803112.19964, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 88.5918140411377, \"count\": 1, \"min\": 88.5918140411377, \"max\": 88.5918140411377}}}\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:52 INFO 140171572815680] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:52 INFO 140171572815680] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1705803112.1997242, \"EndTime\": 1705803112.2036808, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 3.9103031158447266, \"count\": 1, \"min\": 3.9103031158447266, \"max\": 3.9103031158447266}}}\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:52 INFO 140171572815680] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[01/21/2024 02:11:52 INFO 140171572815680] No test data passed, skipping evaluation.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1705803112.2037368, \"EndTime\": 1705803112.2078826, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 5.5828094482421875, \"count\": 1, \"min\": 5.5828094482421875, \"max\": 5.5828094482421875}, \"totaltime\": {\"sum\": 273400.80094337463, \"count\": 1, \"min\": 273400.80094337463, \"max\": 273400.80094337463}}}\u001b[0m\n",
      "\n",
      "2024-01-21 02:12:55 Uploading - Uploading generated training model\n",
      "2024-01-21 02:13:06 Completed - Training job completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: forecasting-deepar-2024-01-21-02-13-37-406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seconds: 570\n",
      "Billable seconds: 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name forecast-endpoint\n",
      "INFO:sagemaker:Creating endpoint with name forecast-endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'strftime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 93\u001b[0m\n\u001b[1;32m     87\u001b[0m end_date \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(start_date) \u001b[38;5;241m+\u001b[39m pd\u001b[38;5;241m.\u001b[39mDateOffset(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Preparar los datos para la predicción\u001b[39;00m\n\u001b[1;32m     90\u001b[0m prediction_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m'\u001b[39m: [\n\u001b[1;32m     92\u001b[0m         {\n\u001b[0;32m---> 93\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mstart_date\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mSZ\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     94\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m: df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Utilizar los valores de 'target' generados\u001b[39;00m\n\u001b[1;32m     95\u001b[0m         }\n\u001b[1;32m     96\u001b[0m     ]\n\u001b[1;32m     97\u001b[0m }\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Realizar la predicción\u001b[39;00m\n\u001b[1;32m    100\u001b[0m result \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mpredict(prediction_data)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'strftime'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import Session\n",
    "from sagemaker import image_uris\n",
    "\n",
    "# Definir las credenciales de AWS\n",
    "AWS_ACCESS_KEY = 'AKIAVISDIRYZU7LYYR45'\n",
    "AWS_SECRET_KEY = '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'\n",
    "AWS_REGION = 'us-east-1'  # Cambia a tu región\n",
    "\n",
    "# Configurar el rol de ejecución de SageMaker\n",
    "role = get_execution_role()\n",
    "\n",
    "# Crear cliente de SageMaker\n",
    "sagemaker_client = boto3.client('sagemaker', aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY, region_name=AWS_REGION)\n",
    "\n",
    "# Nombre del dataset\n",
    "dataset_name = 'sales_forecast_dataset'\n",
    "\n",
    "# Crear el dataset para SageMaker Forecast\n",
    "df['start'] = df['EventDate'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "# Crear el campo 'target' que incluye los valores de los próximos 7 días\n",
    "df['target'] = df.apply(lambda row: df[(df['EventDate'] > row['EventDate']) & (df['EventDate'] <= row['EventDate'] + pd.DateOffset(days=7))]['TotalSold'].tolist(), axis=1)\n",
    "\n",
    "df[['start', 'target']].to_json('sales_data.json', orient='records', lines=True)\n",
    "\n",
    "# Modificar el archivo JSON para eliminar la información de la zona horaria\n",
    "with open('sales_data.json', 'r') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "# Eliminar la información de la zona horaria\n",
    "data = [line.replace('Z', '') for line in data]\n",
    "\n",
    "# Guardar el archivo modificado\n",
    "with open('sales_data.json', 'w') as file:\n",
    "    file.writelines(data)\n",
    "\n",
    "# Crear una instancia del cliente de S3\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Subir el archivo de datos a S3\n",
    "s3_client.upload_file('sales_data.json', 'modelaciondk', 'forecast/sales_data.json')\n",
    "\n",
    "# Configuración del predictor\n",
    "image_uri = image_uris.retrieve('forecasting-deepar', AWS_REGION, version='1.0')\n",
    "container = f'{image_uri}'\n",
    "\n",
    "# Crear el estimador de SageMaker Forecast\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    sagemaker_session=Session(),\n",
    ")\n",
    "\n",
    "# Configurar el nombre del trabajo de entrenamiento de manera personalizada\n",
    "training_job_name = f'sales-forecast-training-job-{int(time.time())}'\n",
    "\n",
    "# Definir las configuraciones del modelo\n",
    "estimator.set_hyperparameters(\n",
    "    time_freq='D',\n",
    "    context_length=10,\n",
    "    num_cells=50,\n",
    "    num_layers=2,\n",
    "    likelihood='gaussian',\n",
    "    epochs=10,\n",
    "    prediction_length=5\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "estimator.fit({'train': f's3://modelaciondk/forecast/sales_data.json'}, job_name=training_job_name)\n",
    "\n",
    "# Crear el predictor\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    endpoint_name='forecast-endpoint'\n",
    ")\n",
    "\n",
    "# Realizar predicciones para los próximos 7 días\n",
    "start_date = df['start'].max()\n",
    "end_date = pd.to_datetime(start_date) + pd.DateOffset(days=7)\n",
    "\n",
    "# Preparar los datos para la predicción\n",
    "prediction_data = {\n",
    "    'instances': [\n",
    "        {\n",
    "            'start': start_date.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            'target': df['target'].iloc[0]  # Utilizar los valores de 'target' generados\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Realizar la predicción\n",
    "result = predictor.predict(prediction_data)\n",
    "forecast_df = pd.DataFrame(result['predictions'][0]['quantiles']['0.5'], columns=['TotalSold'])\n",
    "forecast_df.index = pd.date_range(start=start_date, periods=7, freq='D')\n",
    "forecast_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107fe004-ea2f-4a20-9f09-b6af9dc29908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

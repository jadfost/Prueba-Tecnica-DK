{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ae9f80-6593-4251-a371-d358c173fe09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importé las bibliotecas necesarias\n",
    "import boto3  # Para interactuar con AWS (Amazon Web Services)\n",
    "import pandas as pd  # Para el análisis de datos\n",
    "from io import StringIO  # Para leer y escribir cadenas como archivos\n",
    "\n",
    "# Creé una instancia del cliente de S3\n",
    "s3_client = boto3.client('s3')  # Inicialicé el cliente de S3 para interactuar con el servicio de almacenamiento de Amazon S3\n",
    "\n",
    "# Descargué el archivo CSV desde S3\n",
    "csv_file = s3_client.get_object(Bucket='modelaciondk', Key='tickitdb/merge_data.csv')['Body'].read().decode('utf-8')\n",
    "# Utilicé el cliente de S3 para obtener el objeto (archivo) desde el bucket 'modelaciondk' y la clave 'tickitdb/merge_data.csv'\n",
    "# Leí el contenido del archivo, lo decodifiqué del formato binario y lo almacené en la variable csv_file\n",
    "\n",
    "# Convertí el CSV a DataFrame con nombres de columnas asignados\n",
    "column_names = ['User', 'LastName', 'Email', 'EventName', 'EventLocation', 'EventDate', 'Quantity', 'TotalSold']\n",
    "df = pd.read_csv(StringIO(csv_file), header=None, names=column_names)\n",
    "# Utilicé pandas para leer el contenido del archivo CSV almacenado en csv_file\n",
    "# Asigné nombres a las columnas del DataFrame según la lista column_names\n",
    "\n",
    "# Aseguré de que la columna de fecha fuera del tipo datetime\n",
    "df['EventDate'] = pd.to_datetime(df['EventDate'])\n",
    "# Convertí la columna 'EventDate' del DataFrame al tipo de dato datetime, facilitando el manejo de fechas en análisis posteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "006f7c91-8aea-47cb-90ea-8e00d5e2980d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.34.19)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.19 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3) (1.34.19)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.19->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.19->boto3) (1.26.18)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.19->boto3) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38df09e6-32c3-4aca-982f-d8fbc0f7f5c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "WARNING:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.0.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: sales-forecast-training-job-1705800173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-21 01:22:54 Starting - Starting the training job...\n",
      "2024-01-21 01:23:18 Starting - Preparing the instances for training.........\n",
      "2024-01-21 01:24:47 Downloading - Downloading input data...\n",
      "2024-01-21 01:25:17 Downloading - Downloading the training image.....................\n",
      "2024-01-21 01:28:34 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34mRunning custom environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:97: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m[01/21/2024 01:28:59 INFO 139938889799488] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[01/21/2024 01:28:59 INFO 139938889799488] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'context_length': '10', 'epochs': '10', 'likelihood': 'gaussian', 'num_cells': '50', 'num_layers': '2', 'prediction_length': '5', 'time_freq': 'D'}\u001b[0m\n",
      "\u001b[34m[01/21/2024 01:28:59 INFO 139938889799488] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'gaussian', 'mini_batch_size': '128', 'num_cells': '50', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'context_length': '10', 'epochs': '10', 'prediction_length': '5', 'time_freq': 'D'}\u001b[0m\n",
      "\u001b[34mProcess 7 is a worker.\u001b[0m\n",
      "\u001b[34m[01/21/2024 01:28:59 INFO 139938889799488] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[01/21/2024 01:28:59 INFO 139938889799488] random_seed is None\u001b[0m\n",
      "\u001b[34m[01/21/2024 01:28:59 INFO 139938889799488] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/sales_data.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[01/21/2024 01:28:59 INFO 139938889799488] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/sales_data.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[01/21/2024 01:29:02 ERROR 139938889799488] Customer Error: You have configured context_length=10 and prediction_length=5, but the average time series length in the data is only 1.0. Consider decreasing the context and/or prediction length such that average time series length is larger than the sum of context length and prediction length.\u001b[0m\n",
      "\n",
      "2024-01-21 01:29:21 Uploading - Uploading generated training model\n",
      "2024-01-21 01:29:21 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job sales-forecast-training-job-1705800173: Failed. Reason: ClientError: You have configured context_length=10 and prediction_length=5, but the average time series length in the data is only 1.0. Consider decreasing the context and/or prediction length such that average time series length is larger than the sum of context length and prediction length., exit code: 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 73\u001b[0m\n\u001b[1;32m     62\u001b[0m estimator\u001b[38;5;241m.\u001b[39mset_hyperparameters(\n\u001b[1;32m     63\u001b[0m     time_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     64\u001b[0m     context_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m     prediction_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     70\u001b[0m )\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms3://modelaciondk/forecast/sales_data.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_job_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Crear el predictor\u001b[39;00m\n\u001b[1;32m     76\u001b[0m predictor \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mdeploy(\n\u001b[1;32m     77\u001b[0m     initial_instance_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     78\u001b[0m     instance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml.m4.xlarge\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     79\u001b[0m     endpoint_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforecast-endpoint\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     80\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/estimator.py:1341\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/estimator.py:2677\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2675\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2677\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2678\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:5506\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   5485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   5486\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   5487\u001b[0m \n\u001b[1;32m   5488\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5504\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   5505\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5506\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:7634\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(sagemaker_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   7631\u001b[0m             last_profiler_rule_statuses \u001b[38;5;241m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   7633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 7634\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   7636\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:7687\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   7681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   7682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   7683\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   7684\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   7685\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   7686\u001b[0m     )\n\u001b[0;32m-> 7687\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   7688\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   7689\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   7690\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   7691\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job sales-forecast-training-job-1705800173: Failed. Reason: ClientError: You have configured context_length=10 and prediction_length=5, but the average time series length in the data is only 1.0. Consider decreasing the context and/or prediction length such that average time series length is larger than the sum of context length and prediction length., exit code: 2"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import Session\n",
    "from sagemaker import image_uris\n",
    "\n",
    "# Definir las credenciales de AWS\n",
    "AWS_ACCESS_KEY = 'AKIAVISDIRYZU7LYYR45'\n",
    "AWS_SECRET_KEY = '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'\n",
    "AWS_REGION = 'us-east-1'  # Cambia a tu región\n",
    "\n",
    "# Configurar el rol de ejecución de SageMaker\n",
    "role = get_execution_role()\n",
    "\n",
    "# Crear cliente de SageMaker\n",
    "sagemaker_client = boto3.client('sagemaker', aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY, region_name=AWS_REGION)\n",
    "\n",
    "# Nombre del dataset\n",
    "dataset_name = 'sales_forecast_dataset'\n",
    "\n",
    "# Crear el dataset para SageMaker Forecast\n",
    "df['start'] = df['EventDate'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "df['target'] = df['TotalSold'].apply(lambda x: [x])\n",
    "df[['start', 'target']].to_json('sales_data.json', orient='records', lines=True)\n",
    "\n",
    "# Modificar el archivo JSON para eliminar la información de la zona horaria\n",
    "with open('sales_data.json', 'r') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "# Eliminar la información de la zona horaria\n",
    "data = [line.replace('Z', '') for line in data]\n",
    "\n",
    "# Guardar el archivo modificado\n",
    "with open('sales_data.json', 'w') as file:\n",
    "    file.writelines(data)\n",
    "\n",
    "# Crear una instancia del cliente de S3\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Subir el archivo de datos a S3\n",
    "s3_client.upload_file('sales_data.json', 'modelaciondk', 'forecast/sales_data.json')\n",
    "\n",
    "# Configuración del predictor\n",
    "image_uri = image_uris.retrieve('forecasting-deepar', AWS_REGION, version='1.0')\n",
    "container = f'{image_uri}'\n",
    "\n",
    "# Crear el estimador de SageMaker Forecast\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    sagemaker_session=Session(),\n",
    ")\n",
    "\n",
    "# Configurar el nombre del trabajo de entrenamiento de manera personalizada\n",
    "training_job_name = f'sales-forecast-training-job-{int(time.time())}'\n",
    "\n",
    "# Definir las configuraciones del modelo\n",
    "estimator.set_hyperparameters(\n",
    "    time_freq='D',\n",
    "    context_length=10,\n",
    "    num_cells=50,\n",
    "    num_layers=2,\n",
    "    likelihood='gaussian',\n",
    "    epochs=10,\n",
    "    prediction_length=5\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "estimator.fit({'train': f's3://modelaciondk/forecast/sales_data.json'}, job_name=training_job_name)\n",
    "\n",
    "# Crear el predictor\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    endpoint_name='forecast-endpoint'\n",
    ")\n",
    "\n",
    "# Realizar predicciones para los próximos 7 días\n",
    "start_date = df['start'].max()\n",
    "end_date = pd.to_datetime(start_date) + pd.DateOffset(days=7)\n",
    "\n",
    "# Preparar los datos para la predicción\n",
    "prediction_data = {\n",
    "    'instances': [\n",
    "        {\n",
    "            'start': start_date.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            'target': df['TotalSold'].tolist()\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Realizar la predicción\n",
    "result = predictor.predict(prediction_data)\n",
    "forecast_df = pd.DataFrame(result['predictions'][0]['quantiles']['0.5'], columns=['TotalSold'])\n",
    "forecast_df.index = pd.date_range(start=start_date, periods=7, freq='D')\n",
    "forecast_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107fe004-ea2f-4a20-9f09-b6af9dc29908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
